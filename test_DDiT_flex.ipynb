{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import os.path\n",
    "import gc\n",
    "from itertools import chain\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import data\n",
    "import losses\n",
    "import sampling\n",
    "import graph_lib\n",
    "import noise_lib\n",
    "import utils\n",
    "from model import SEDD_flex\n",
    "from model.ema import ExponentialMovingAverage\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "def setup(rank, world_size, port):\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = str(port)\n",
    "\n",
    "    # initialize the process group\n",
    "    dist.init_process_group(\n",
    "        \"nccl\", rank=rank, world_size=world_size, timeout=datetime.timedelta(minutes=30)\n",
    "    )\n",
    "\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "def run_multiprocess(rank, world_size, cfg, port):\n",
    "    try:\n",
    "        setup(rank, world_size, port)\n",
    "        _run(rank, world_size, cfg)\n",
    "    finally:\n",
    "        cleanup()\n",
    "\n",
    "def get_random_batch(cfg):\n",
    "    # get random data\n",
    "    batch_size = cfg.training.batch_size\n",
    "    \n",
    "    # Current limitation that the total sequnce length must be divisible by 128\n",
    "    sentence_lengths = [random.randint(1, 1024) for _ in range(batch_size - 1)]\n",
    "    print(\"sentence lengths:\", sentence_lengths)\n",
    "    total = sum(sentence_lengths)\n",
    "    sentence_lengths.append(128 - total % 128)\n",
    "    total = sum(sentence_lengths)\n",
    "    print(\"actual total:\", total)\n",
    "\n",
    "    ragged_tensors = [torch.randint(0, cfg.tokens, l, device=\"cuda\") for l in sentence_lengths]\n",
    "    batch = torch.nested.nested_tensor(\n",
    "        ragged_tensors, layout=torch.jagged\n",
    "    )\n",
    "\n",
    "def _run(rank, world_size, cfg):\n",
    "    torch.cuda.set_device(rank)\n",
    "    work_dir = cfg.work_dir\n",
    "\n",
    "    # Create directories for experimental logs\n",
    "    sample_dir = os.path.join(work_dir, \"samples\")\n",
    "    checkpoint_dir = os.path.join(work_dir, \"checkpoints\")\n",
    "    checkpoint_meta_dir = os.path.join(work_dir, \"checkpoints-meta\", \"checkpoint.pth\")\n",
    "    if rank == 0:\n",
    "        utils.makedirs(sample_dir)\n",
    "        utils.makedirs(checkpoint_dir)\n",
    "        utils.makedirs(os.path.dirname(checkpoint_meta_dir))\n",
    "\n",
    "    # logging\n",
    "    if rank == 0:\n",
    "        logger = utils.get_logger(os.path.join(work_dir, \"logs\"))\n",
    "    def mprint(msg):\n",
    "        if rank == 0:\n",
    "            logger.info(msg)\n",
    "\n",
    "    mprint(work_dir)\n",
    "    mprint(cfg)\n",
    "    device = torch.device(f\"cuda:{rank}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if device.type == \"cuda\":\n",
    "        mprint(\"Found {} CUDA devices.\".format(torch.cuda.device_count()))\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            props = torch.cuda.get_device_properties(i)\n",
    "            mprint(\n",
    "                \"{} \\t Memory: {:.2f}GB\".format(\n",
    "                    props.name, props.total_memory / (1024 ** 3)\n",
    "                )\n",
    "            )\n",
    "    else:\n",
    "        mprint(\"WARNING: Using device {}\".format(device))\n",
    "    mprint(f\"Found {os.cpu_count()} total number of CPUs.\")\n",
    "\n",
    "    # build token graph\n",
    "    graph = graph_lib.get_graph(cfg, device)\n",
    "    \n",
    "    # build score model\n",
    "    score_model = SEDD_flex(cfg).to(device)\n",
    "    score_model = DDP(score_model, device_ids=[rank], static_graph=True, find_unused_parameters=True)\n",
    "\n",
    "    num_parameters = sum(p.numel() for p in score_model.parameters())\n",
    "    mprint(f\"Number of parameters in the model: {num_parameters}\")\n",
    "\n",
    "    ema = ExponentialMovingAverage(\n",
    "        score_model.parameters(), decay=cfg.training.ema)\n",
    "    mprint(score_model)\n",
    "    mprint(f\"EMA: {ema}\")\n",
    "\n",
    "    # build noise\n",
    "    noise = noise_lib.get_noise(cfg).to(device)\n",
    "    noise = DDP(noise, device_ids=[rank], static_graph=True)\n",
    "    sampling_eps = 1e-5\n",
    "\n",
    "\n",
    "    # build optimization state\n",
    "    optimizer = losses.get_optimizer(cfg, chain(score_model.parameters(), noise.parameters()))\n",
    "    mprint(f\"Optimizer: {optimizer}\")\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    mprint(f\"Scaler: {scaler}\")\n",
    "    state = dict(optimizer=optimizer, scaler=scaler, model=score_model, noise=noise, ema=ema, step=0) \n",
    "\n",
    "\n",
    "    # load in state\n",
    "    state = utils.restore_checkpoint(checkpoint_meta_dir, state, device)\n",
    "    initial_step = int(state['step'])\n",
    "\n",
    "    \n",
    "    # load in tokenizer\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "\n",
    "    # Build one-step training and evaluation functions\n",
    "    optimize_fn = losses.optimization_manager(cfg)\n",
    "    train_step_fn = losses.get_step_fn(noise, graph, True, optimize_fn, cfg.training.accum)\n",
    "    eval_step_fn = losses.get_step_fn(noise, graph, False, optimize_fn, cfg.training.accum)\n",
    "\n",
    "\n",
    "    if cfg.training.snapshot_sampling: # TODO: support for different length sampling\n",
    "        sampling_shape = (cfg.training.batch_size // (cfg.ngpus * cfg.training.accum), cfg.model.length)\n",
    "        sampling_fn = sampling.get_sampling_fn(cfg, graph, noise, sampling_shape, sampling_eps, device)\n",
    "\n",
    "    num_train_steps = cfg.training.n_iters\n",
    "    mprint(f\"Starting training loop at step {initial_step}.\")\n",
    "\n",
    "\n",
    "    while state['step'] < num_train_steps + 1:\n",
    "        step = state['step']\n",
    "\n",
    "        batch = get_random_batch(cfg)\n",
    "\n",
    "        loss = train_step_fn(state, batch)\n",
    "\n",
    "        # flag to see if there was movement ie a full batch got computed\n",
    "        if step != state['step']:\n",
    "            if step % cfg.training.log_freq == 0:\n",
    "                dist.all_reduce(loss)\n",
    "                loss /= world_size\n",
    "\n",
    "                mprint(\"step: %d, training_loss: %.5e\" % (step, loss.item()))\n",
    "            \n",
    "            if step % cfg.training.snapshot_freq_for_preemption == 0 and rank == 0:\n",
    "                utils.save_checkpoint(checkpoint_meta_dir, state)\n",
    "\n",
    "            if step % cfg.training.eval_freq == 0:\n",
    "                    \n",
    "                eval_batch = get_random_batch(cfg)\n",
    "                eval_loss = eval_step_fn(state, eval_batch)\n",
    "\n",
    "                dist.all_reduce(eval_loss)\n",
    "                eval_loss /= world_size\n",
    "\n",
    "                mprint(\"step: %d, evaluation_loss: %.5e\" % (step, eval_loss.item()))\n",
    "\n",
    "            if step > 0 and step % cfg.training.snapshot_freq == 0 or step == num_train_steps:\n",
    "                # Save the checkpoint.\n",
    "                save_step = step // cfg.training.snapshot_freq\n",
    "                if rank == 0:\n",
    "                    utils.save_checkpoint(os.path.join(\n",
    "                        checkpoint_dir, f'checkpoint_{save_step}.pth'), state)\n",
    "\n",
    "                # Generate and save samples\n",
    "                if cfg.training.snapshot_sampling:\n",
    "                    mprint(f\"Generating text at step: {step}\")\n",
    "\n",
    "                    this_sample_dir = os.path.join(sample_dir, \"iter_{}\".format(step))\n",
    "                    utils.makedirs(this_sample_dir)\n",
    "\n",
    "                    ema.store(score_model.parameters())\n",
    "                    ema.copy_to(score_model.parameters())\n",
    "                    sample = sampling_fn(score_model)\n",
    "                    ema.restore(score_model.parameters())\n",
    "\n",
    "                    sentences = tokenizer.batch_decode(sample)\n",
    "                    \n",
    "                    file_name = os.path.join(this_sample_dir, f\"sample_{rank}.txt\")\n",
    "                    with open(file_name, 'w') as file:\n",
    "                        for sentence in sentences:\n",
    "                            file.write(sentence + \"\\n\")\n",
    "                            file.write(\"============================================================================================\\n\")\n",
    "\n",
    "                    if cfg.eval.perplexity:\n",
    "                        with torch.no_grad():\n",
    "                            eval_model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\").to(device).eval()\n",
    "                            batches = sample.shape[0] // cfg.eval.perplexity_batch_size\n",
    "                            total_perplexity = 0\n",
    "                            for i in range(batches):\n",
    "                                s = sample[i * cfg.eval.perplexity_batch_size:(i + 1) * cfg.eval.perplexity_batch_size]\n",
    "                                loss, logits = eval_model(s, labels=s)[:2]\n",
    "                                logits = logits.transpose(-1, -2)\n",
    "                                perplexity = F.cross_entropy(logits[..., :-1], s[..., 1:], reduction=\"none\").mean(dim=-1).exp().mean()\n",
    "                                total_perplexity += perplexity\n",
    "                            total_perplexity /= batches\n",
    "                            dist.all_reduce(total_perplexity)\n",
    "                            total_perplexity /= world_size\n",
    "                            mprint(f\"Generative Perplexity at step: {step}. Perplexity: {total_perplexity:.3f}.\")\n",
    "\n",
    "                            del eval_model, logits, loss\n",
    "\n",
    "                    dist.barrier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [--help] [--hydra-help] [--version]\n",
      "                             [--cfg {job,hydra,all}] [--resolve]\n",
      "                             [--package PACKAGE] [--run] [--multirun]\n",
      "                             [--shell-completion] [--config-path CONFIG_PATH]\n",
      "                             [--config-name CONFIG_NAME]\n",
      "                             [--config-dir CONFIG_DIR]\n",
      "                             [--experimental-rerun EXPERIMENTAL_RERUN]\n",
      "                             [--info [{all,config,defaults,defaults-tree,plugins,searchpath}]]\n",
      "                             [overrides ...]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/home/kkj/.local/share/jupyter/runtime/kernel-v3b407d4db983062d594ce4545f2894ffc9448d677.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kkj/axolotl/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Training and evaluation\"\"\"\n",
    "\n",
    "import hydra\n",
    "import os\n",
    "import numpy as np\n",
    "import run_train\n",
    "import utils\n",
    "import torch.multiprocessing as mp\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "from hydra.types import RunMode\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "\n",
    "@hydra.main(version_base=None, config_path=\"configs\", config_name=\"config\")\n",
    "def main(cfg):\n",
    "    ngpus = cfg.ngpus\n",
    "    if \"load_dir\" in cfg:\n",
    "        hydra_cfg_path = os.path.join(cfg.load_dir, \".hydra/hydra.yaml\")\n",
    "        hydra_cfg = OmegaConf.load(hydra_cfg_path).hydra\n",
    "\n",
    "        cfg = utils.load_hydra_config_from_run(cfg.load_dir)\n",
    "        \n",
    "        work_dir = cfg.work_dir\n",
    "        utils.makedirs(work_dir)\n",
    "    else:\n",
    "        hydra_cfg = HydraConfig.get()\n",
    "        work_dir = hydra_cfg.run.dir if hydra_cfg.mode == RunMode.RUN else os.path.join(hydra_cfg.sweep.dir, hydra_cfg.sweep.subdir)\n",
    "        utils.makedirs(work_dir)\n",
    "\n",
    "    with open_dict(cfg):\n",
    "        cfg.ngpus = ngpus\n",
    "        cfg.work_dir = work_dir\n",
    "        cfg.wandb_name = os.path.basename(os.path.normpath(work_dir))\n",
    "\n",
    "\t# Run the training pipeline\n",
    "    port = int(np.random.randint(10000, 20000))\n",
    "    logger = utils.get_logger(os.path.join(work_dir, \"logs\"))\n",
    "\n",
    "    hydra_cfg = HydraConfig.get()\n",
    "    if hydra_cfg.mode != RunMode.RUN:\n",
    "        logger.info(f\"Run id: {hydra_cfg.job.id}\")\n",
    "\n",
    "    try:\n",
    "        mp.set_start_method(\"forkserver\")\n",
    "        mp.spawn(run_train.run_multiprocess, args=(ngpus, cfg, port), nprocs=ngpus, join=True)\n",
    "    except Exception as e:\n",
    "        logger.critical(e, exc_info=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
