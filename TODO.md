[] Flash attention with mask - Maybe using FlexAttention (maybe not "FlexAttention requires that all sequence lengths be a multiple of 128 - this will be addressed soon." https://pytorch.org/blog/flexattention/)
[] Clustered DataLoader (as in ProtDiffusion)