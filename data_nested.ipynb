{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import axolotl.data_nested as data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load testdata\n",
    "time0 = time.time() \n",
    "train_set = data.get_dataset('/mnt/e/datasets/IPR036736_90_grouped/train')\n",
    "# train_set = data.get_dataset('/mnt/e/datasets/UniRef50_grouped/train')\n",
    "time1 = time.time()\n",
    "print(f\"Dataset loaded in {time1 - time0:.2f} seconds\")\n",
    "\n",
    "lengths = train_set[\"length\"]\n",
    "time2 = time.time()\n",
    "# print(f\"Lengths: {lengths}\")\n",
    "print(f\"lengths retrieved in {time2 - time1:.2f} seconds\")\n",
    "\n",
    "cluster_sizes = train_set[\"cluster_size\"]\n",
    "time3 = time.time()\n",
    "print(f\"Cluster sizes: {cluster_sizes}\")\n",
    "print(f\"Cluster sizes retrieved in {time3 - time2:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"/home/kkj/axolotl/datasets/IPR036736_90_grouped/train\"\n",
    "BATCH_SIZE = 16\n",
    "CTX_LEN = 2048\n",
    "NUM_GPUS = 8\n",
    "EPOCHS = 10\n",
    "SEED = 42\n",
    "START_EPOCH = 0\n",
    "\n",
    "SAMPLERS = {\n",
    "    \"Simple Distributed Batch Sampler\": lambda lengths, cluster_sizes, rank: data.SimpleDistributedBatchSampler(\n",
    "        dataset=train_set,\n",
    "        length_key=\"length\",\n",
    "        cluster_size_key=\"cluster_size\",\n",
    "        max_length=CTX_LEN,\n",
    "        total_length=BATCH_SIZE * CTX_LEN,\n",
    "        num_replicas=NUM_GPUS,\n",
    "        rank=rank,\n",
    "        seed=SEED,\n",
    "        epoch=START_EPOCH,\n",
    "        shuffle=True,  # Shuffle batches for each epoch\n",
    "),\n",
    "    \"Multipack QuadraticAttention\": lambda lengths, cluster_sizes, rank: data.MultipackDistributedBatchSampler(lengths=lengths, \n",
    "                                                                                                cluster_sizes=cluster_sizes,\n",
    "                                                                                                max_length=CTX_LEN,\n",
    "                                                                                                total_length=BATCH_SIZE * CTX_LEN, \n",
    "                                                                                                num_replicas=NUM_GPUS, \n",
    "                                                                                                rank=rank,\n",
    "                                                                                                seed=SEED,\n",
    "                                                                                                epoch=START_EPOCH,\n",
    "),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sampler correctness & efficiency\n",
    "for sampler_name, sampler_fn in SAMPLERS.items():\n",
    "    print(f\"Sampler {sampler_name}:\")\n",
    "\n",
    "    tot_len = 0\n",
    "    tot_maxlen = 0\n",
    "    tot_batches = 0\n",
    "    avg_sql2lag = 0\n",
    "    max_sql2lag = 0\n",
    "\n",
    "    # Detailed timing for sampler creation\n",
    "    start_time = time.time()\n",
    "    samplers = [sampler_fn(lengths=lengths, cluster_sizes=cluster_sizes, rank=rank) for rank in range(NUM_GPUS)]\n",
    "    creation_time = time.time() - start_time\n",
    "    print(f\"Sampler creation time: {creation_time*1000:.2f}ms\")\n",
    "    \n",
    "    # Try to get batch counts (might not work for streaming samplers)\n",
    "    try:\n",
    "        batch_count_start = time.time()\n",
    "        batch_counts = [sampler.num_batches() for sampler in samplers if hasattr(sampler, 'num_batches')]\n",
    "        batch_count_time = time.time() - batch_count_start\n",
    "        print(\"Batch count for ranks:\", batch_counts)\n",
    "        print(f\"Batch counting time: {batch_count_time:.2f}s\")\n",
    "    except:\n",
    "        print(\"Batch counting not available (streaming sampler)\")\n",
    "    \n",
    "    print(\"\")\n",
    "\n",
    "    # Time each epoch\n",
    "    for epoch in range(START_EPOCH, EPOCHS + START_EPOCH):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        batches = []\n",
    "        sqlen = [[] for _ in range(NUM_GPUS)]\n",
    "        olen = [[] for _ in range(NUM_GPUS)]\n",
    "\n",
    "        # Time epoch setting\n",
    "        set_epoch_start = time.time()\n",
    "        for rank, sampler in enumerate(samplers):\n",
    "            sampler.set_epoch(epoch)\n",
    "        set_epoch_time = time.time() - set_epoch_start\n",
    "        \n",
    "        # Time batch iteration\n",
    "        iteration_start = time.time()\n",
    "        cluster_selection_seed = (SEED + epoch)\n",
    "        \n",
    "        batch_processing_times = []\n",
    "        for rank, sampler in enumerate(samplers):\n",
    "            rank_start = time.time()\n",
    "            \n",
    "            for batch_idx, batch in enumerate(sampler):\n",
    "                batch_start = time.time()\n",
    "                \n",
    "                batches.extend(batch)\n",
    "                cluster_selection_idx = [cluster_selection_seed % cluster_sizes[x] for x in batch]\n",
    "\n",
    "                # Check constraints\n",
    "                overall_len = sum([min(lengths[x][y], CTX_LEN) for x, y in zip(batch, cluster_selection_idx)])\n",
    "                square_len = sum([lengths[x][y] ** 2 for x, y in zip(batch, cluster_selection_idx)])\n",
    "                # assert overall_len <= BATCH_SIZE * CTX_LEN, f\"Overall length {overall_len} exceeds maximum {BATCH_SIZE * CTX_LEN} for batch {batch} at rank {rank} in epoch {epoch}\"\n",
    "\n",
    "                # Add stats\n",
    "                tot_len += overall_len\n",
    "                tot_batches += 1\n",
    "\n",
    "                # square len\n",
    "                sqlen[rank].append(square_len)\n",
    "                olen[rank].append(overall_len)\n",
    "                \n",
    "                batch_time = time.time() - batch_start\n",
    "                batch_processing_times.append(batch_time)\n",
    "            \n",
    "            rank_time = time.time() - rank_start\n",
    "            if epoch == START_EPOCH:  # Only print for first epoch\n",
    "                print(f\"  Rank {rank} processing time: {rank_time:.2f}m\")\n",
    "        \n",
    "        iteration_time = time.time() - iteration_start\n",
    "        \n",
    "        # # Time statistics computation\n",
    "        # stats_start = time.time()\n",
    "        # tot_maxlen += np.sum(np.max(olen, axis=0))\n",
    "\n",
    "        # sqlen = np.array(sqlen)\n",
    "        # sqlag = np.max(sqlen, axis=0) - np.min(sqlen, axis=0)\n",
    "\n",
    "        # avg_sql2lag += np.sqrt(np.mean(sqlag))\n",
    "        # max_sql2lag += np.sqrt(np.max(sqlag))\n",
    "\n",
    "        # Check overall unique\n",
    "        batches.sort()\n",
    "        assert batches == list(set(batches))  # Unique\n",
    "        # stats_time = time.time() - stats_start\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        if epoch == START_EPOCH:  # Print detailed timing for first epoch\n",
    "            print(f\"  Epoch {epoch} timing breakdown:\")\n",
    "            print(f\"    Set epoch: {set_epoch_time:.2f}s\")\n",
    "            print(f\"    Batch iteration: {iteration_time:.2f}s\")\n",
    "            # print(f\"    Statistics: {stats_time*1000:.2f}ms\")\n",
    "            print(f\"    Total epoch: {epoch_time:.2f}s\")\n",
    "            if batch_processing_times:\n",
    "                print(f\"    Avg batch processing: {np.mean(batch_processing_times):.2f}s\")\n",
    "                print(f\"    Min/Max batch processing: {np.min(batch_processing_times):.2f}s / {np.max(batch_processing_times):.2f}s\")\n",
    "\n",
    "    # # Check efficiency\n",
    "    # print(f\"L^2 lag avg: {avg_sql2lag / EPOCHS:.0f} max: {max_sql2lag / EPOCHS:.0f}\")\n",
    "    # print(f\"Efficiency: {tot_len / (tot_batches * CTX_LEN * BATCH_SIZE) * 100:.2f}%\")\n",
    "    # print(f\"Utilization: {tot_len / (tot_maxlen * NUM_GPUS) * 100:.2f}%\")\n",
    "    # print(\"==========\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed timing tests for sampler performance\n",
    "print(\"=== DETAILED TIMING TESTS ===\\n\")\n",
    "\n",
    "def time_sampler_parts(sampler_name, sampler_fn, rank=0):\n",
    "    print(f\"Testing {sampler_name} (Rank {rank}):\")\n",
    "    \n",
    "    # Test sampler creation time\n",
    "    start_time = time.time()\n",
    "    sampler = sampler_fn(lengths=lengths, cluster_sizes=cluster_sizes, rank=rank)\n",
    "    creation_time = time.time() - start_time\n",
    "    print(f\"  Sampler creation: {creation_time*1000:.2f}ms\")\n",
    "    \n",
    "    # Test epoch setting time\n",
    "    start_time = time.time()\n",
    "    sampler.set_epoch(0)\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"  Set epoch: {epoch_time*1000:.2f}ms\")\n",
    "    \n",
    "    # Test iteration initialization time\n",
    "    start_time = time.time()\n",
    "    iterator = iter(sampler)\n",
    "    init_time = time.time() - start_time\n",
    "    print(f\"  Iterator init: {init_time*1000:.2f}ms\")\n",
    "    \n",
    "    # Test first batch generation time\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        first_batch = next(iterator)\n",
    "        first_batch_time = time.time() - start_time\n",
    "        print(f\"  First batch: {first_batch_time*1000:.2f}ms (size: {len(first_batch)})\")\n",
    "    except StopIteration:\n",
    "        print(\"  First batch: No batches generated\")\n",
    "        return\n",
    "    \n",
    "    # Test subsequent batch generation times\n",
    "    batch_times = []\n",
    "    batch_sizes = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    batch_count = 1  # Already got first batch\n",
    "    for batch in iterator:\n",
    "        batch_time = time.time()\n",
    "        batch_times.append((batch_time - start_time) * 1000)\n",
    "        batch_sizes.append(len(batch))\n",
    "        start_time = batch_time\n",
    "        batch_count += 1\n",
    "        if batch_count >= 10:  # Test first 10 batches\n",
    "            break\n",
    "    \n",
    "    if batch_times:\n",
    "        print(f\"  Avg batch time (2-10): {np.mean(batch_times):.2f}ms\")\n",
    "        print(f\"  Min/Max batch time: {np.min(batch_times):.2f}ms / {np.max(batch_times):.2f}ms\")\n",
    "        print(f\"  Avg batch size: {np.mean(batch_sizes):.1f}\")\n",
    "        print(f\"  Min/Max batch size: {min(batch_sizes)} / {max(batch_sizes)}\")\n",
    "    \n",
    "    # Test collate function time if available\n",
    "    if hasattr(sampler, 'collate_fn'):\n",
    "        # Create a mock batch using actual dataset samples\n",
    "        mock_batch = train_set[first_batch]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        collated = sampler.collate_fn(mock_batch)\n",
    "        collate_time = time.time() - start_time\n",
    "        print(f\"  Collate time (first batch): {collate_time*1000:.2f}ms\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Test each sampler\n",
    "for sampler_name, sampler_fn in SAMPLERS.items():\n",
    "    time_sampler_parts(sampler_name, sampler_fn, rank=0)\n",
    "\n",
    "print(\"=== MEMORY USAGE TEST ===\")\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "def test_memory_usage(sampler_name, sampler_fn, rank=0):\n",
    "    print(f\"Testing memory usage for {sampler_name}:\")\n",
    "    \n",
    "    # Get initial memory\n",
    "    process = psutil.Process()\n",
    "    initial_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    # Create sampler\n",
    "    sampler = sampler_fn(lengths=lengths, cluster_sizes=cluster_sizes, rank=rank)\n",
    "    after_creation = process.memory_info().rss / 1024 / 1024\n",
    "    \n",
    "    # Generate some batches\n",
    "    sampler.set_epoch(0)\n",
    "    batches = []\n",
    "    for i, batch in enumerate(iter(sampler)):\n",
    "        batches.append(batch)\n",
    "        if i >= 50:  # Generate 50 batches\n",
    "            break\n",
    "    \n",
    "    after_batches = process.memory_info().rss / 1024 / 1024\n",
    "    \n",
    "    print(f\"  Initial memory: {initial_memory:.1f}MB\")\n",
    "    print(f\"  After creation: {after_creation:.1f}MB (+{after_creation-initial_memory:.1f}MB)\")\n",
    "    print(f\"  After 50 batches: {after_batches:.1f}MB (+{after_batches-initial_memory:.1f}MB)\")\n",
    "    print(f\"  Generated {len(batches)} batches\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del sampler, batches\n",
    "    gc.collect()\n",
    "    print()\n",
    "\n",
    "# Test memory usage for each sampler\n",
    "for sampler_name, sampler_fn in SAMPLERS.items():\n",
    "    test_memory_usage(sampler_name, sampler_fn, rank=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "CTX_LEN = 1024\n",
    "NUM_GPUS = 1\n",
    "EPOCHS = 10\n",
    "SEED = 42\n",
    "START_EPOCH = 0\n",
    "\n",
    "SAMPLERS = {\n",
    "    \"Simple Distributed Batch Sampler\": lambda lengths, cluster_sizes, rank: data.SimpleDistributedBatchSampler(\n",
    "        dataset=train_set,\n",
    "        length_key=\"length\",\n",
    "        cluster_size_key=\"cluster_size\",\n",
    "        max_length=CTX_LEN,\n",
    "        total_length=BATCH_SIZE * CTX_LEN,\n",
    "        num_replicas=NUM_GPUS,\n",
    "        rank=rank,\n",
    "        seed=SEED,\n",
    "        epoch=START_EPOCH,\n",
    "        shuffle=True,  # Shuffle batches for each epoch\n",
    "),\n",
    "    \"Multipack QuadraticAttention\": lambda lengths, cluster_sizes, rank: data.MultipackDistributedBatchSampler(lengths=lengths, \n",
    "                                                                                                cluster_sizes=cluster_sizes,\n",
    "                                                                                                max_length=CTX_LEN,\n",
    "                                                                                                total_length=BATCH_SIZE * CTX_LEN, \n",
    "                                                                                                num_replicas=NUM_GPUS, \n",
    "                                                                                                rank=rank,\n",
    "                                                                                                seed=SEED,\n",
    "                                                                                                epoch=START_EPOCH,\n",
    "),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from /mnt/e/datasets/IPR036736_90_grouped/train\n",
      "Loading dataset from /mnt/e/datasets/IPR036736_90_grouped/test\n",
      "Testing first batch generation:\n",
      "  First train batch: 0.94s\n",
      "  First train batch tokens: tensor([    0,    81,   169,   323,   399,   489,  1513,  2537,  3561,  4585,\n",
      "         5528,  5618,  6642,  7666,  8690,  9714, 10738, 11762, 12786, 13810,\n",
      "        14834, 15855, 16879, 17885, 17976, 19000, 20024, 20162, 20259, 21283,\n",
      "        21384, 22408, 22517, 22602, 23626, 24650, 24731, 25706, 26623, 27647,\n",
      "        27733, 28757, 29781, 30601, 31625, 32649])\n",
      "  First test batch: 3.86s\n",
      "  First train batch tokens: tensor([    0,   192,   604,   682,  1706,  2730,  3754,  4778,  4859,  5883,\n",
      "         5969,  6993,  8017,  9041,  9200,  9273,  9355, 10379, 11403, 12427,\n",
      "        13439, 13518, 14228, 15252, 16276, 16639, 16714, 17738, 18762, 19786,\n",
      "        20810, 21834, 21920, 22564, 23588, 24612, 25636, 25742, 26766, 27790,\n",
      "        27906, 27994, 28071, 28193, 28376, 28641, 29665, 30267, 31291, 32315])\n",
      "\n",
      "Testing subsequent batch generation:\n",
      "  Batch 10: tensor([    0,  1024,  2048,  3072,  3165,  4189,  4531,  5555,  5637,  6487,\n",
      "         7506,  8530,  8615,  9639, 10663, 11687, 12711, 13553, 14577, 15601,\n",
      "        16625, 17649, 18545, 18632, 19656, 20680, 21704, 22728, 22809, 23669,\n",
      "        23766, 24790, 25814, 26838, 26918, 27010, 27088, 27189, 28213, 29237,\n",
      "        30261, 31285, 31371, 32395])\n",
      "  Batch 20: tensor([    0,  1024,  1185,  2209,  3233,  4257,  4342,  4436,  5460,  6484,\n",
      "         7508,  8532,  9556, 10470, 10549, 11573, 12597, 13621, 14645, 14728,\n",
      "        14817, 14893, 14970, 15994, 17018, 17546, 18570, 18684, 19708, 20732,\n",
      "        21756, 22780, 23804, 24759, 25783, 26807, 27831, 28490, 29308, 30332,\n",
      "        30406, 30934, 31958])\n",
      "  Batch 30: tensor([    0,  1024,  2048,  3072,  3163,  3260,  3338,  4362,  5386,  5481,\n",
      "         5559,  6583,  7268,  8292,  9316, 10002, 10637, 11661, 12685, 12763,\n",
      "        12853, 13877, 14901, 15925, 16017, 17041, 17119, 18143, 18231, 19107,\n",
      "        20131, 21155, 22179, 22258, 22347, 23371, 24395, 25419, 26443, 27467,\n",
      "        28491, 29515, 30539, 31563, 31648, 32672])\n",
      "  Batch 40: tensor([    0,  1024,  2048,  3072,  3547,  3616,  4640,  5664,  6688,  7712,\n",
      "         8736,  8891,  9915, 10939, 11963, 12987, 14011, 15035, 16059, 17083,\n",
      "        17999, 18371, 19395, 20419, 21443, 22295, 23319, 24200, 25224, 26248,\n",
      "        26349, 27373, 28397, 28487, 29511, 29651, 30675, 30755, 31626, 31710,\n",
      "        31791, 32516])\n",
      "  Batch 50: tensor([    0,  1024,  2048,  3072,  4096,  5120,  5214,  5301,  6325,  7349,\n",
      "         7434,  8458,  9482, 10506, 11530, 11615, 12639, 12738, 12826, 13850,\n",
      "        14874, 15898, 16006, 16090, 17114, 17235, 17370, 18394, 18469, 19493,\n",
      "        20517, 21541, 22565, 23589, 24613, 25637, 26203, 27227, 28251, 29275,\n",
      "        29357, 30376, 31400, 32424, 32489])\n",
      "\n",
      "Total batches generated: 50\n",
      "Average batch time: 0.15s\n",
      "Min/Max batch time: 0.12s / 0.21s\n"
     ]
    }
   ],
   "source": [
    "import axolotl.data_nested as data\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "DATASET_TRAIN = \"/mnt/e/datasets/IPR036736_90_grouped/train\"\n",
    "DATASET_TEST = \"/mnt/e/datasets/IPR036736_90_grouped/test\"\n",
    "\n",
    "train_loader, test_loader = data.get_dataloaders(\n",
    "    train_batch_size=BATCH_SIZE,\n",
    "    valid_batch_size=BATCH_SIZE,\n",
    "    ngpus=NUM_GPUS,\n",
    "    accum=1,\n",
    "    train_path=DATASET_TRAIN,\n",
    "    valid_path=DATASET_TEST,\n",
    "    max_length=CTX_LEN,\n",
    "    drop_last=True,\n",
    "    num_workers=4,\n",
    "    distributed=False,\n",
    "    seed=42, # Seed for picking the data in the clusters and making torch generators.\n",
    "    epoch=0,\n",
    "    shuffle_each_epoch=True,\n",
    ")\n",
    "\n",
    "train_iter = iter(train_loader)\n",
    "test_iter = iter(test_loader)\n",
    "\n",
    "# Test first batch generation\n",
    "print(\"Testing first batch generation:\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    first_train_batch = next(train_iter)\n",
    "    first_train_time = time.time() - start_time\n",
    "    print(f\"  First train batch: {first_train_time:.2f}s\")\n",
    "    print(f\"  First train batch tokens: {first_train_batch['input_ids'].offsets()}\")\n",
    "except:\n",
    "    print(\"  First train batch: No batches generated\")\n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    first_test_batch = next(test_iter)\n",
    "    first_test_time = time.time() - start_time\n",
    "    print(f\"  First test batch: {first_test_time:.2f}s\")\n",
    "    print(f\"  First train batch tokens: {first_test_batch['input_ids'].offsets()}\")\n",
    "except:\n",
    "    print(\"  First test batch: No batches generated\")\n",
    "\n",
    "# Test subsequent batch generation\n",
    "print(\"\\nTesting subsequent batch generation:\")\n",
    "batch_count = 0\n",
    "times = []\n",
    "while True:\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        batch = next(train_iter)\n",
    "        batch_time = time.time() - start_time\n",
    "        times.append(batch_time)\n",
    "        batch_count += 1\n",
    "        if batch_count % 10 == 0:  # Print every 10 batches\n",
    "            print(f\"  Batch {batch_count}: {batch['input_ids'].offsets()}\")\n",
    "    except StopIteration:\n",
    "        print(\"  No more batches available\")\n",
    "        break\n",
    "\n",
    "    if batch_count >= 50:  # Limit to 50 batches\n",
    "        break\n",
    "\n",
    "print(f\"\\nTotal batches generated: {batch_count}\")\n",
    "if times:\n",
    "    print(f\"Average batch time: {np.mean(times):.2f}s\")\n",
    "    print(f\"Min/Max batch time: {np.min(times):.2f}s / {np.max(times):.2f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
