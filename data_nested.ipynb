{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import axolotl.data_nested as data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load testdata\n",
    "time0 = time.time() \n",
    "train_set = data.get_dataset('/mnt/e/datasets/IPR036736_90_grouped/train')\n",
    "# train_set = data.get_dataset('/mnt/e/datasets/UniRef50_grouped/train')\n",
    "time1 = time.time()\n",
    "print(f\"Dataset loaded in {time1 - time0:.2f} seconds\")\n",
    "\n",
    "lengths = train_set[\"length\"]\n",
    "time2 = time.time()\n",
    "# print(f\"Lengths: {lengths}\")\n",
    "print(f\"lengths retrieved in {time2 - time1:.2f} seconds\")\n",
    "\n",
    "cluster_sizes = train_set[\"cluster_size\"]\n",
    "time3 = time.time()\n",
    "print(f\"Cluster sizes: {cluster_sizes}\")\n",
    "print(f\"Cluster sizes retrieved in {time3 - time2:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"/home/kkj/axolotl/datasets/IPR036736_90_grouped/train\"\n",
    "BATCH_SIZE = 16\n",
    "CTX_LEN = 2048\n",
    "NUM_GPUS = 8\n",
    "EPOCHS = 10\n",
    "SEED = 42\n",
    "START_EPOCH = 0\n",
    "\n",
    "SAMPLERS = {\n",
    "    \"Simple Distributed Batch Sampler\": lambda lengths, cluster_sizes, rank: data.SimpleDistributedBatchSampler(\n",
    "        dataset=train_set,\n",
    "        length_key=\"length\",\n",
    "        cluster_size_key=\"cluster_size\",\n",
    "        max_length=CTX_LEN,\n",
    "        total_length=BATCH_SIZE * CTX_LEN,\n",
    "        num_replicas=NUM_GPUS,\n",
    "        rank=rank,\n",
    "        seed=SEED,\n",
    "        epoch=START_EPOCH,\n",
    "        shuffle=True,  # Shuffle batches for each epoch\n",
    "),\n",
    "    \"Multipack QuadraticAttention\": lambda lengths, cluster_sizes, rank: data.MultipackDistributedBatchSampler(lengths=lengths, \n",
    "                                                                                                cluster_sizes=cluster_sizes,\n",
    "                                                                                                max_length=CTX_LEN,\n",
    "                                                                                                total_length=BATCH_SIZE * CTX_LEN, \n",
    "                                                                                                num_replicas=NUM_GPUS, \n",
    "                                                                                                rank=rank,\n",
    "                                                                                                seed=SEED,\n",
    "                                                                                                epoch=START_EPOCH,\n",
    "),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sampler correctness & efficiency\n",
    "for sampler_name, sampler_fn in SAMPLERS.items():\n",
    "    print(f\"Sampler {sampler_name}:\")\n",
    "\n",
    "    tot_len = 0\n",
    "    tot_maxlen = 0\n",
    "    tot_batches = 0\n",
    "    avg_sql2lag = 0\n",
    "    max_sql2lag = 0\n",
    "\n",
    "    # Detailed timing for sampler creation\n",
    "    start_time = time.time()\n",
    "    samplers = [sampler_fn(lengths=lengths, cluster_sizes=cluster_sizes, rank=rank) for rank in range(NUM_GPUS)]\n",
    "    creation_time = time.time() - start_time\n",
    "    print(f\"Sampler creation time: {creation_time*1000:.2f}ms\")\n",
    "    \n",
    "    # Try to get batch counts (might not work for streaming samplers)\n",
    "    try:\n",
    "        batch_count_start = time.time()\n",
    "        batch_counts = [sampler.num_batches() for sampler in samplers if hasattr(sampler, 'num_batches')]\n",
    "        batch_count_time = time.time() - batch_count_start\n",
    "        print(\"Batch count for ranks:\", batch_counts)\n",
    "        print(f\"Batch counting time: {batch_count_time:.2f}s\")\n",
    "    except:\n",
    "        print(\"Batch counting not available (streaming sampler)\")\n",
    "    \n",
    "    print(\"\")\n",
    "\n",
    "    # Time each epoch\n",
    "    for epoch in range(START_EPOCH, EPOCHS + START_EPOCH):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        batches = []\n",
    "        sqlen = [[] for _ in range(NUM_GPUS)]\n",
    "        olen = [[] for _ in range(NUM_GPUS)]\n",
    "\n",
    "        # Time epoch setting\n",
    "        set_epoch_start = time.time()\n",
    "        for rank, sampler in enumerate(samplers):\n",
    "            sampler.set_epoch(epoch)\n",
    "        set_epoch_time = time.time() - set_epoch_start\n",
    "        \n",
    "        # Time batch iteration\n",
    "        iteration_start = time.time()\n",
    "        cluster_selection_seed = (SEED + epoch)\n",
    "        \n",
    "        batch_processing_times = []\n",
    "        for rank, sampler in enumerate(samplers):\n",
    "            rank_start = time.time()\n",
    "            \n",
    "            for batch_idx, batch in enumerate(sampler):\n",
    "                batch_start = time.time()\n",
    "                \n",
    "                batches.extend(batch)\n",
    "                cluster_selection_idx = [cluster_selection_seed % cluster_sizes[x] for x in batch]\n",
    "\n",
    "                # Check constraints\n",
    "                overall_len = sum([min(lengths[x][y], CTX_LEN) for x, y in zip(batch, cluster_selection_idx)])\n",
    "                square_len = sum([lengths[x][y] ** 2 for x, y in zip(batch, cluster_selection_idx)])\n",
    "                # assert overall_len <= BATCH_SIZE * CTX_LEN, f\"Overall length {overall_len} exceeds maximum {BATCH_SIZE * CTX_LEN} for batch {batch} at rank {rank} in epoch {epoch}\"\n",
    "\n",
    "                # Add stats\n",
    "                tot_len += overall_len\n",
    "                tot_batches += 1\n",
    "\n",
    "                # square len\n",
    "                sqlen[rank].append(square_len)\n",
    "                olen[rank].append(overall_len)\n",
    "                \n",
    "                batch_time = time.time() - batch_start\n",
    "                batch_processing_times.append(batch_time)\n",
    "            \n",
    "            rank_time = time.time() - rank_start\n",
    "            if epoch == START_EPOCH:  # Only print for first epoch\n",
    "                print(f\"  Rank {rank} processing time: {rank_time:.2f}m\")\n",
    "        \n",
    "        iteration_time = time.time() - iteration_start\n",
    "        \n",
    "        # # Time statistics computation\n",
    "        # stats_start = time.time()\n",
    "        # tot_maxlen += np.sum(np.max(olen, axis=0))\n",
    "\n",
    "        # sqlen = np.array(sqlen)\n",
    "        # sqlag = np.max(sqlen, axis=0) - np.min(sqlen, axis=0)\n",
    "\n",
    "        # avg_sql2lag += np.sqrt(np.mean(sqlag))\n",
    "        # max_sql2lag += np.sqrt(np.max(sqlag))\n",
    "\n",
    "        # Check overall unique\n",
    "        batches.sort()\n",
    "        assert batches == list(set(batches))  # Unique\n",
    "        # stats_time = time.time() - stats_start\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        \n",
    "        if epoch == START_EPOCH:  # Print detailed timing for first epoch\n",
    "            print(f\"  Epoch {epoch} timing breakdown:\")\n",
    "            print(f\"    Set epoch: {set_epoch_time:.2f}s\")\n",
    "            print(f\"    Batch iteration: {iteration_time:.2f}s\")\n",
    "            # print(f\"    Statistics: {stats_time*1000:.2f}ms\")\n",
    "            print(f\"    Total epoch: {epoch_time:.2f}s\")\n",
    "            if batch_processing_times:\n",
    "                print(f\"    Avg batch processing: {np.mean(batch_processing_times):.2f}s\")\n",
    "                print(f\"    Min/Max batch processing: {np.min(batch_processing_times):.2f}s / {np.max(batch_processing_times):.2f}s\")\n",
    "\n",
    "    # # Check efficiency\n",
    "    # print(f\"L^2 lag avg: {avg_sql2lag / EPOCHS:.0f} max: {max_sql2lag / EPOCHS:.0f}\")\n",
    "    # print(f\"Efficiency: {tot_len / (tot_batches * CTX_LEN * BATCH_SIZE) * 100:.2f}%\")\n",
    "    # print(f\"Utilization: {tot_len / (tot_maxlen * NUM_GPUS) * 100:.2f}%\")\n",
    "    # print(\"==========\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed timing tests for sampler performance\n",
    "print(\"=== DETAILED TIMING TESTS ===\\n\")\n",
    "\n",
    "def time_sampler_parts(sampler_name, sampler_fn, rank=0):\n",
    "    print(f\"Testing {sampler_name} (Rank {rank}):\")\n",
    "    \n",
    "    # Test sampler creation time\n",
    "    start_time = time.time()\n",
    "    sampler = sampler_fn(lengths=lengths, cluster_sizes=cluster_sizes, rank=rank)\n",
    "    creation_time = time.time() - start_time\n",
    "    print(f\"  Sampler creation: {creation_time*1000:.2f}ms\")\n",
    "    \n",
    "    # Test epoch setting time\n",
    "    start_time = time.time()\n",
    "    sampler.set_epoch(0)\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"  Set epoch: {epoch_time*1000:.2f}ms\")\n",
    "    \n",
    "    # Test iteration initialization time\n",
    "    start_time = time.time()\n",
    "    iterator = iter(sampler)\n",
    "    init_time = time.time() - start_time\n",
    "    print(f\"  Iterator init: {init_time*1000:.2f}ms\")\n",
    "    \n",
    "    # Test first batch generation time\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        first_batch = next(iterator)\n",
    "        first_batch_time = time.time() - start_time\n",
    "        print(f\"  First batch: {first_batch_time*1000:.2f}ms (size: {len(first_batch)})\")\n",
    "    except StopIteration:\n",
    "        print(\"  First batch: No batches generated\")\n",
    "        return\n",
    "    \n",
    "    # Test subsequent batch generation times\n",
    "    batch_times = []\n",
    "    batch_sizes = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    batch_count = 1  # Already got first batch\n",
    "    for batch in iterator:\n",
    "        batch_time = time.time()\n",
    "        batch_times.append((batch_time - start_time) * 1000)\n",
    "        batch_sizes.append(len(batch))\n",
    "        start_time = batch_time\n",
    "        batch_count += 1\n",
    "        if batch_count >= 10:  # Test first 10 batches\n",
    "            break\n",
    "    \n",
    "    if batch_times:\n",
    "        print(f\"  Avg batch time (2-10): {np.mean(batch_times):.2f}ms\")\n",
    "        print(f\"  Min/Max batch time: {np.min(batch_times):.2f}ms / {np.max(batch_times):.2f}ms\")\n",
    "        print(f\"  Avg batch size: {np.mean(batch_sizes):.1f}\")\n",
    "        print(f\"  Min/Max batch size: {min(batch_sizes)} / {max(batch_sizes)}\")\n",
    "    \n",
    "    # Test collate function time if available\n",
    "    if hasattr(sampler, 'collate_fn'):\n",
    "        # Create a mock batch using actual dataset samples\n",
    "        mock_batch = train_set[first_batch]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        collated = sampler.collate_fn(mock_batch)\n",
    "        collate_time = time.time() - start_time\n",
    "        print(f\"  Collate time (first batch): {collate_time*1000:.2f}ms\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Test each sampler\n",
    "for sampler_name, sampler_fn in SAMPLERS.items():\n",
    "    time_sampler_parts(sampler_name, sampler_fn, rank=0)\n",
    "\n",
    "print(\"=== MEMORY USAGE TEST ===\")\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "def test_memory_usage(sampler_name, sampler_fn, rank=0):\n",
    "    print(f\"Testing memory usage for {sampler_name}:\")\n",
    "    \n",
    "    # Get initial memory\n",
    "    process = psutil.Process()\n",
    "    initial_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    # Create sampler\n",
    "    sampler = sampler_fn(lengths=lengths, cluster_sizes=cluster_sizes, rank=rank)\n",
    "    after_creation = process.memory_info().rss / 1024 / 1024\n",
    "    \n",
    "    # Generate some batches\n",
    "    sampler.set_epoch(0)\n",
    "    batches = []\n",
    "    for i, batch in enumerate(iter(sampler)):\n",
    "        batches.append(batch)\n",
    "        if i >= 50:  # Generate 50 batches\n",
    "            break\n",
    "    \n",
    "    after_batches = process.memory_info().rss / 1024 / 1024\n",
    "    \n",
    "    print(f\"  Initial memory: {initial_memory:.1f}MB\")\n",
    "    print(f\"  After creation: {after_creation:.1f}MB (+{after_creation-initial_memory:.1f}MB)\")\n",
    "    print(f\"  After 50 batches: {after_batches:.1f}MB (+{after_batches-initial_memory:.1f}MB)\")\n",
    "    print(f\"  Generated {len(batches)} batches\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del sampler, batches\n",
    "    gc.collect()\n",
    "    print()\n",
    "\n",
    "# Test memory usage for each sampler\n",
    "for sampler_name, sampler_fn in SAMPLERS.items():\n",
    "    test_memory_usage(sampler_name, sampler_fn, rank=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "CTX_LEN = 1024\n",
    "NUM_GPUS = 1\n",
    "EPOCHS = 10\n",
    "SEED = 42\n",
    "START_EPOCH = 0\n",
    "\n",
    "SAMPLERS = {\n",
    "    \"Simple Distributed Batch Sampler\": lambda lengths, cluster_sizes, rank: data.SimpleDistributedBatchSampler(\n",
    "        dataset=train_set,\n",
    "        length_key=\"length\",\n",
    "        cluster_size_key=\"cluster_size\",\n",
    "        max_length=CTX_LEN,\n",
    "        total_length=BATCH_SIZE * CTX_LEN,\n",
    "        num_replicas=NUM_GPUS,\n",
    "        rank=rank,\n",
    "        seed=SEED,\n",
    "        epoch=START_EPOCH,\n",
    "        shuffle=True,  # Shuffle batches for each epoch\n",
    "),\n",
    "    \"Multipack QuadraticAttention\": lambda lengths, cluster_sizes, rank: data.MultipackDistributedBatchSampler(lengths=lengths, \n",
    "                                                                                                cluster_sizes=cluster_sizes,\n",
    "                                                                                                max_length=CTX_LEN,\n",
    "                                                                                                total_length=BATCH_SIZE * CTX_LEN, \n",
    "                                                                                                num_replicas=NUM_GPUS, \n",
    "                                                                                                rank=rank,\n",
    "                                                                                                seed=SEED,\n",
    "                                                                                                epoch=START_EPOCH,\n",
    "),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from /mnt/e/datasets/IPR036736_90_grouped/train\n",
      "Loading dataset from /mnt/e/datasets/IPR036736_90_grouped/test\n",
      "Testing first batch generation:\n",
      "  First train batch: 0.76s\n",
      "  First train batch tokens: tensor([    0,    81,   169,   323,   399,   489,  1513,  2537,  3561,  4585,\n",
      "         5528,  5618,  6642,  7666,  8690,  9714, 10738, 11762, 12786, 13810,\n",
      "        14834, 15855, 16879, 17885, 17976, 19000, 20024, 20162, 20259, 21283,\n",
      "        21384, 22408, 22517, 22602, 23626, 24650, 24731, 25706, 26623, 27647,\n",
      "        27733, 28757, 29781, 30601, 31625, 32649])\n",
      "  First test batch: 0.83s\n",
      "  First train batch tokens: tensor([    0,   192,   604,   682,  1706,  2730,  3754,  4778,  4859,  5883,\n",
      "         5969,  6993,  8017,  9041,  9200,  9273,  9355, 10379, 11403, 12427,\n",
      "        13439, 13518, 14228, 15252, 16276, 16639, 16714, 17738, 18762, 19786,\n",
      "        20810, 21834, 21920, 22564, 23588, 24612, 25636, 25742, 26766, 27790,\n",
      "        27906, 27994, 28071, 28193, 28376, 28641, 29665, 30267, 31291, 32315])\n",
      "\n",
      "Testing subsequent batch generation:\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  1\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  2\n",
      "  Batch 100: tensor([    0,  1024,  1120,  1210,  1993,  3017,  3922,  4004,  5028,  6052,\n",
      "         7076,  8100,  9124, 10148, 10974, 11998, 13022, 14046, 15070, 16094,\n",
      "        17118, 18142, 18554, 19578, 20602, 20675, 21699, 22723, 23747, 24771,\n",
      "        24856, 25880, 26904, 27928, 28952, 29041, 30065, 31089, 31180, 32204])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  3\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  4\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  5\n",
      "  Batch 200: tensor([    0,  1024,  1931,  2955,  3032,  4046,  5070,  6094,  6217,  7241,\n",
      "         7323,  8347,  8425,  9449, 10473, 11497, 12521, 13063, 14087, 14174,\n",
      "        15198, 16222, 16877, 17901, 18925, 19546, 19634, 20658, 20751, 21775,\n",
      "        22799, 23823, 24847, 25871, 26858, 27882, 28906, 28987, 30011, 30097,\n",
      "        30182, 30259, 31283, 32307])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  6\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  7\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  8\n",
      "  Batch 300: tensor([    0,  1024,  1628,  1715,  1798,  2662,  3686,  3770,  3853,  4877,\n",
      "         5901,  6925,  7949,  8035,  9059,  9139,  9228, 10252, 10340, 10427,\n",
      "        11451, 12475, 12654, 13003, 14027, 14112, 14205, 15229, 16134, 16748,\n",
      "        17772, 17975, 18053, 18976, 19061, 20085, 20447, 21471, 22495, 22631,\n",
      "        23655, 23735, 24759, 25783, 26807, 27409, 28433, 29457, 29537, 29633,\n",
      "        30657, 31681, 32705, 32787])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  9\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  10\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  11\n",
      "  Batch 400: tensor([    0,  1024,  1948,  2972,  3996,  5020,  6044,  7052,  8076,  8158,\n",
      "         9182, 10206, 11230, 12254, 12383, 12484, 12566, 12647, 12724, 13748,\n",
      "        13827, 14851, 15875, 16597, 16677, 17701, 17852, 17938, 18962, 19085,\n",
      "        19215, 19827, 20851, 20946, 21046, 22070, 22167, 23191, 24215, 24295,\n",
      "        24373, 25397, 26421, 27445, 27547, 27628, 28044, 29068, 29154, 29238,\n",
      "        30262, 30342, 30425, 31449, 32473])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  12\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  13\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  14\n",
      "  Batch 500: tensor([    0,  1024,  2048,  2598,  3622,  4646,  5670,  5747,  5830,  5915,\n",
      "         6939,  7963,  8040,  8130,  8751,  8831,  9855, 10879, 11903, 12927,\n",
      "        13951, 14975, 15070, 16094, 17118, 18023, 18840, 19864, 20888, 20973,\n",
      "        21851, 22833, 22919, 23943, 24555, 25579, 26603, 27627, 28651, 29560,\n",
      "        29638, 29740, 29839, 29916, 30940, 31896, 31986])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  15\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  16\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  17\n",
      "  Batch 600: tensor([    0,  1024,  2048,  2136,  3160,  3244,  4093,  5117,  6141,  6248,\n",
      "         6328,  7154,  8178,  8263,  8343,  9367,  9445,  9630, 10654, 11678,\n",
      "        12702, 12807, 12887, 12964, 13988, 15012, 15874, 16898, 17551, 18575,\n",
      "        19599, 19686, 19775, 20799, 21823, 22847, 23871, 24436, 24534, 25558,\n",
      "        26172, 27196, 28220, 28300, 28384, 28504, 29528, 30395, 31419, 31580,\n",
      "        32604, 32706])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  18\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  19\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  20\n",
      "  Batch 700: tensor([    0,  1024,  1097,  2083,  2170,  3194,  3282,  4306,  5330,  6354,\n",
      "         7368,  8392,  9416,  9499, 10523, 10617, 11641, 11764, 11848, 11933,\n",
      "        12957, 13981, 14075, 15099, 15215, 16239, 17263, 18287, 19311, 20335,\n",
      "        20467, 20546, 21570, 22594, 23618, 24642, 25666, 25743, 26767, 26841,\n",
      "        27809, 27980, 29004, 29102, 29187, 30211, 31235, 31321, 32345, 32428])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  21\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  22\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  23\n",
      "  Batch 800: tensor([    0,  1024,  1111,  1282,  1369,  1460,  1923,  2597,  3621,  4517,\n",
      "         5541,  6565,  7589,  7952,  8038,  9062, 10086, 11110, 12134, 13158,\n",
      "        14170, 14248, 15272, 16296, 16391, 17415, 18439, 18533, 18623, 19647,\n",
      "        20671, 20824, 21727, 21807, 22831, 23855, 23934, 24801, 25825, 26849,\n",
      "        27454, 28116, 29140, 30164, 31188, 31281, 31818])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  24\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  25\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  26\n",
      "  Batch 900: tensor([    0,   760,  1784,  2808,  3832,  4856,  5880,  5959,  6983,  8007,\n",
      "         9031,  9113, 10137, 11161, 11524, 11878, 12902, 12998, 13092, 14116,\n",
      "        14396, 15410, 16434, 17458, 18482, 19506, 20530, 21554, 21643, 22667,\n",
      "        22791, 22881, 23905, 24442, 25466, 26490, 27265, 28289, 28362, 29386,\n",
      "        29468, 30492, 30633, 30714, 30799, 31823])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  27\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  28\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  29\n",
      "  Batch 1000: tensor([    0,  1024,  2048,  2133,  2210,  2284,  3308,  4332,  5356,  6289,\n",
      "         7313,  7397,  8421,  9445, 10376, 11400, 12424, 12519, 13543, 14567,\n",
      "        15591, 16615, 17639, 18663, 19687, 20711, 21735, 21813, 22837, 23861,\n",
      "        24885, 25909, 25997, 26085, 27081, 27172, 27259, 28283, 28399, 29423,\n",
      "        29512, 30536, 31560, 32584])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  30\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  31\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  32\n",
      "  Batch 1100: tensor([    0,   967,  1991,  2078,  3102,  4126,  5150,  5273,  6056,  6555,\n",
      "         6636,  7531,  8555,  8638,  9662, 10164, 11188, 11269, 12293, 13317,\n",
      "        13419, 14443, 15467, 16480, 17504, 17584, 18090, 19114, 20138, 20738,\n",
      "        21460, 22484, 22594, 23618, 24642, 25666, 25753, 25838, 26862, 27065,\n",
      "        28089, 29113, 29747, 30605, 30684, 31708, 32732])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  33\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  34\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  35\n",
      "  Batch 1200: tensor([    0,   891,   979,  2003,  3027,  3405,  4429,  5453,  6477,  7501,\n",
      "         7579,  8452,  8533,  9557,  9651,  9736,  9829, 10853, 11877, 12901,\n",
      "        13925, 14949, 15973, 16997, 17067, 18091, 19115, 20139, 20225, 20299,\n",
      "        20385, 20473, 21497, 21587, 22611, 22707, 23079, 24103, 24990, 25069,\n",
      "        26093, 27117, 28141, 29165, 29403, 29531, 30555, 31579, 32293])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  36\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  37\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  38\n",
      "  Batch 1300: tensor([    0,  1024,  1122,  1205,  1294,  2318,  3342,  4238,  5133,  6157,\n",
      "         7181,  7276,  8300,  8591,  9615,  9987, 11011, 12035, 13059, 14083,\n",
      "        15107, 15310, 15979, 16071, 17095, 17182, 17258, 17757, 17837, 18861,\n",
      "        19885, 20909, 21933, 22957, 23981, 25005, 25976, 26054, 27078, 28060,\n",
      "        28199, 28918, 29020, 30044, 30122, 30199, 31223, 32247, 32330, 32426])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  39\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  40\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  41\n",
      "  Batch 1400: tensor([    0,  1024,  1114,  1714,  2202,  2296,  3320,  3404,  4428,  5452,\n",
      "         6476,  7500,  8524,  9548,  9633,  9707,  9788,  9888, 10912, 11936,\n",
      "        12960, 13984, 15008, 16032, 17056, 18080, 18176, 18266, 18356, 19380,\n",
      "        20271, 21295, 22144, 23168, 24192, 24285, 25309, 25397, 25487, 26511,\n",
      "        27140, 27228, 28252, 28336, 29360, 29444, 29525, 30549, 30630, 30717,\n",
      "        31008, 32032])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  42\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  43\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  44\n",
      "  Batch 1500: tensor([    0,  1024,  2048,  3072,  4096,  4687,  4776,  5800,  5884,  6908,\n",
      "         6994,  8018,  9042,  9728, 10752, 10840, 11864, 12888, 13049, 14073,\n",
      "        15097, 15180, 15271, 16295, 17319, 17407, 17500, 18524, 19548, 20572,\n",
      "        21596, 22375, 23127, 24151, 25175, 25262, 26286, 27310, 28334, 29358,\n",
      "        30382, 30922, 31016, 32040])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  45\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  46\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  47\n",
      "  Batch 1600: tensor([    0,  1024,  1733,  2757,  3781,  4805,  5829,  6853,  6941,  7017,\n",
      "         8041,  8122,  9146, 10049, 11073, 11157, 12181, 12260, 13284, 13549,\n",
      "        13629, 14653, 15677, 15765, 16789, 16885, 16978, 17170, 18194, 19218,\n",
      "        19304, 20328, 20416, 21440, 21517, 22541, 23565, 24589, 25613, 26637,\n",
      "        26719, 27261, 27341, 28365, 28452, 28534, 29351, 30375, 30458, 31482,\n",
      "        32506])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  48\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  49\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  50\n",
      "  Batch 1700: tensor([    0,    88,  1112,  2136,  3160,  4184,  4274,  4362,  4503,  4605,\n",
      "         4718,  5742,  6282,  6422,  7265,  8289,  9276,  9341, 10365, 11389,\n",
      "        12413, 12502, 13526, 14550, 15574, 15654, 16292, 16383, 17407, 18431,\n",
      "        18930, 19026, 19700, 20724, 21748, 21850, 22874, 23898, 24922, 25008,\n",
      "        26032, 26199, 27223, 27303, 28327, 28421, 29445, 30303, 30380, 30461,\n",
      "        31310, 31397, 32421, 32507])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  51\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  52\n",
      "  Batch 1800: tensor([    0,  1024,  1911,  2935,  3025,  4049,  5073,  5156,  6180,  6271,\n",
      "         6339,  6417,  7441,  8465,  8542,  8619,  9643,  9749, 10630, 11654,\n",
      "        12678, 13702, 14726, 15750, 16774, 17798, 18822, 18933, 19467, 20491,\n",
      "        21515, 21598, 22622, 23646, 23727, 24751, 25775, 26799, 27823, 27905,\n",
      "        28929, 29021, 29116, 29275, 30299, 30415, 30509, 31521, 32545, 32619])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  53\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  54\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  55\n",
      "  Batch 1900: tensor([    0,  1024,  2048,  2921,  3945,  4803,  4877,  4957,  5981,  6760,\n",
      "         7784,  8808,  8904,  8990,  9835, 10859, 10947, 11437, 12461, 13485,\n",
      "        14407, 15431, 16189, 17213, 18237, 19261, 20285, 20370, 21394, 22418,\n",
      "        23442, 23531, 23612, 24554, 25578, 25853, 26498, 26626, 27444, 28468,\n",
      "        28544, 29568, 29651, 30675, 30752, 31776, 32640])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  56\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  57\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  58\n",
      "  Batch 2000: tensor([    0,  1024,  1782,  2806,  3830,  4854,  5474,  6136,  7160,  7249,\n",
      "         8273,  8360,  8483,  9507,  9590,  9887, 10911, 10994, 11076, 12032,\n",
      "        13056, 13144, 14168, 14263, 15287, 16311, 17335, 18359, 18456, 19480,\n",
      "        19567, 19752, 20776, 21194, 22218, 22309, 23333, 23420, 24444, 24529,\n",
      "        24617, 25628, 26652, 26738, 27762, 28786, 28888, 29912, 30936, 31016,\n",
      "        32040, 32131])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  59\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  60\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  61\n",
      "  Batch 2100: tensor([    0,  1024,  1114,  2097,  3046,  4070,  4154,  5178,  5797,  5887,\n",
      "         6901,  7925,  8949,  9973, 10059, 10156, 11180, 12025, 12099, 12171,\n",
      "        13195, 14219, 15243, 16267, 16355, 16449, 16725, 16813, 17708, 18732,\n",
      "        19756, 20780, 21804, 22828, 22925, 23009, 23092, 24116, 24262, 25107,\n",
      "        26131, 26213, 27237, 27543, 27622, 28309, 28390, 29414, 30438, 30519,\n",
      "        31543, 32567])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  62\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  63\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  64\n",
      "  Batch 2200: tensor([    0,  1024,  1113,  2137,  2363,  3387,  4411,  5435,  6459,  6648,\n",
      "         7672,  7753,  7978,  9002, 10026, 11050, 12074, 12151, 13175, 13316,\n",
      "        14340, 15364, 15446, 16470, 17494, 17580, 18604, 19628, 20652, 21676,\n",
      "        22700, 23708, 24680, 25704, 26728, 26813, 27837, 28861, 28944, 29968,\n",
      "        30047, 31071, 31349, 32373])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  65\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  66\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  67\n",
      "  Batch 2300: tensor([    0,  1024,  2048,  2128,  2211,  3235,  4259,  5283,  6307,  7331,\n",
      "         7424,  8448,  8532,  9556, 10580, 11604, 11697, 12721, 13745, 13821,\n",
      "        14845, 14961, 15045, 16069, 17093, 17601, 17682, 17771, 18795, 19819,\n",
      "        19899, 20526, 20667, 20757, 21781, 21862, 21986, 23010, 23088, 23680,\n",
      "        24591, 24677, 24814, 25838, 26776, 27788, 28422, 29393, 29690, 30554,\n",
      "        30667, 30747, 30833, 30910, 31934, 32027])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  68\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  69\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  70\n",
      "  Batch 2400: tensor([    0,  1024,  1185,  2209,  2293,  3317,  4341,  4423,  5447,  6471,\n",
      "         7495,  8519,  9530, 10554, 11578, 11680, 11800, 12824, 13848, 14356,\n",
      "        14850, 15857, 16664, 17688, 17753, 17850, 18788, 19812, 20836, 21860,\n",
      "        21953, 22043, 22136, 22214, 22296, 23141, 23236, 24260, 24346, 24580,\n",
      "        24661, 25685, 25840, 25928, 26952, 27976, 28063, 29087, 29159, 30183,\n",
      "        30271, 31295, 32319, 32504, 32620])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  71\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  72\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  73\n",
      "  Batch 2500: tensor([    0,  1024,  2048,  3072,  4096,  4179,  4264,  4352,  5376,  5467,\n",
      "         6491,  7515,  8539,  8619,  8700,  9458,  9542,  9727, 10751, 11775,\n",
      "        12126, 13150, 13236, 13775, 13914, 13992, 15016, 15110, 16134, 16220,\n",
      "        17244, 17327, 17565, 17647, 17730, 18220, 19244, 20139, 20739, 21763,\n",
      "        22787, 23811, 23888, 24912, 25631, 26655, 27679, 27761, 28785, 28871,\n",
      "        29895, 30919, 31943])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  74\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  75\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  76\n",
      "  Batch 2600: tensor([    0,  1024,  1104,  2128,  3144,  3222,  4246,  5270,  5360,  5451,\n",
      "         5558,  6582,  6698,  7518,  8542,  8638,  9662,  9755, 10779, 11803,\n",
      "        12827, 13735, 14759, 15783, 15863, 16870, 16944, 17968, 18051, 19075,\n",
      "        20099, 20963, 21987, 22086, 22739, 23763, 24641, 25665, 26339, 26421,\n",
      "        26557, 27581, 27674, 27757, 27825, 28734, 28817, 29841, 30335, 30423,\n",
      "        30512, 30591, 30677, 31701, 31837, 31920])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  77\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  78\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  79\n",
      "  Batch 2700: tensor([    0,  1024,  1106,  1183,  1270,  1360,  2384,  3408,  4432,  5456,\n",
      "         5539,  6563,  6644,  7668,  8692,  9716, 10740, 11018, 11614, 11696,\n",
      "        11777, 12801, 13825, 14412, 15436, 16460, 16544, 17568, 18592, 19456,\n",
      "        20480, 21504, 22528, 23552, 23638, 24662, 25686, 25770, 26794, 26930,\n",
      "        27015, 28039, 29063, 30087, 30165, 31189, 32150])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  80\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  81\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  82\n",
      "  Batch 2800: tensor([    0,  1024,  2048,  3072,  3157,  4181,  4269,  5047,  5146,  6170,\n",
      "         6257,  6343,  7367,  8391,  9415,  9636, 10660, 11583, 12607, 13631,\n",
      "        13907, 14931, 15846, 15934, 16023, 17047, 17971, 18124, 19148, 19309,\n",
      "        20333, 21357, 21462, 22486, 23510, 24534, 25558, 25634, 26658, 27471,\n",
      "        28278, 29156, 29279, 30303, 30383, 31407, 32431])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  83\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  84\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  85\n",
      "  Batch 2900: tensor([    0,  1024,  2031,  2116,  3140,  3268,  3821,  4845,  5869,  6893,\n",
      "         7917,  8068,  9092, 10116, 10212, 10298, 10377, 11401, 11487, 11784,\n",
      "        12071, 12194, 12300, 13324, 14348, 14882, 15906, 16930, 17008, 18032,\n",
      "        18128, 19152, 20176, 21200, 22224, 22309, 23333, 23410, 23500, 24524,\n",
      "        25548, 26572, 26661, 27383, 28407, 29431, 30455, 30536, 31560, 31640,\n",
      "        31728, 32752])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  86\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  87\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  88\n",
      "  Batch 3000: tensor([    0,  1024,  2048,  2556,  3580,  4604,  5618,  5746,  6770,  7794,\n",
      "         8818,  8886,  9910, 10288, 10900, 11262, 11392, 12416, 12507, 12589,\n",
      "        13613, 13692, 14716, 15740, 16764, 17788, 18812, 19836, 20860, 20955,\n",
      "        21036, 22060, 22138, 22220, 22305, 22583, 22674, 23698, 24325, 25349,\n",
      "        25431, 26455, 27479, 28503, 29527, 30551, 30857, 30943, 31967, 32052,\n",
      "        32672])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  89\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  90\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  91\n",
      "  Batch 3100: tensor([    0,  1024,  1652,  1733,  1824,  2848,  3872,  4896,  4996,  6020,\n",
      "         6865,  7774,  8033,  9057, 10081, 11105, 12129, 12218, 12838, 13862,\n",
      "        13944, 14038, 15062, 16086, 16167, 16252, 17276, 17364, 18388, 19412,\n",
      "        19494, 19581, 20605, 21629, 22653, 23677, 24701, 25725, 26749, 27731,\n",
      "        28755, 29779, 30803, 30885, 31909, 32933])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  92\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  93\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  94\n",
      "  Batch 3200: tensor([    0,  1024,  1113,  1195,  2219,  2348,  2432,  2522,  3536,  4560,\n",
      "         5584,  6608,  7568,  8592,  8680,  8782,  9806,  9887, 10911, 11935,\n",
      "        12959, 13037, 14048, 15072, 15153, 16177, 16257, 17281, 18211, 19235,\n",
      "        20259, 20346, 20433, 20520, 20606, 21630, 21717, 22741, 23370, 24394,\n",
      "        25418, 25571, 26486, 26563, 26693, 27717, 27803, 28827, 29317, 30114,\n",
      "        30194, 30293, 30409, 30535, 30616, 31640, 31733, 31821, 31906, 32158])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  95\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  96\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  97\n",
      "  Batch 3300: tensor([    0,  1024,  2048,  3072,  4096,  5120,  5211,  5352,  6376,  6963,\n",
      "         7041,  8065,  9089,  9184,  9265,  9348, 10372, 11396, 12420, 12611,\n",
      "        12698, 12786, 12861, 13875, 14899, 15923, 16947, 17971, 18058, 19082,\n",
      "        19166, 20190, 21214, 22072, 23096, 24120, 24199, 25223, 26247, 26861,\n",
      "        26946, 27025, 28049, 28922, 29946, 30970, 31994])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  98\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  99\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  100\n",
      "  Batch 3400: tensor([    0,   119,  1143,  2167,  3191,  4203,  4292,  5316,  6180,  7204,\n",
      "         8228,  9252, 10276, 11300, 12324, 13348, 14372, 15396, 16095, 17102,\n",
      "        18126, 19150, 20174, 21055, 21333, 21417, 22441, 23465, 24489, 24572,\n",
      "        25596, 25681, 26705, 27729, 28753, 29777, 30801, 31454, 32478, 32559,\n",
      "        32652])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  101\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  102\n",
      "  Batch 3500: tensor([    0,  1024,  2048,  3072,  3150,  4174,  5198,  5661,  6685,  7709,\n",
      "         8087,  9000, 10024, 10108, 10192, 11164, 12188, 13212, 14236, 15260,\n",
      "        15337, 16361, 17385, 18409, 19433, 20314, 21338, 22362, 23386, 24410,\n",
      "        24502, 25526, 26550, 27574, 28598, 29622, 29710, 29794, 30818, 31842])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  103\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  104\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  105\n",
      "  Batch 3600: tensor([    0,   619,  1643,  2667,  3691,  4715,  5414,  6438,  7462,  8486,\n",
      "         9510,  9593, 10617, 11641, 11724, 12748, 13629, 13708, 13794, 13873,\n",
      "        13953, 14977, 15265, 16289, 17313, 18158, 18236, 19260, 19345, 19422,\n",
      "        19838, 19917, 20941, 21027, 22051, 22130, 22734, 23694, 24718, 24871,\n",
      "        25895, 26919, 27943, 28967, 29150, 29239, 30127, 31151, 32175, 32260,\n",
      "        32911])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  106\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  107\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  108\n",
      "  Batch 3700: tensor([    0,  1024,  1112,  1198,  2222,  3246,  3932,  4893,  5917,  6839,\n",
      "         7863,  7940,  8821,  9845,  9922, 10946, 11970, 12994, 13083, 14107,\n",
      "        14190, 15214, 15309, 16333, 16420, 17444, 17521, 17610, 18634, 19658,\n",
      "        19736, 20760, 21784, 22808, 22894, 23384, 23471, 23629, 23721, 24093,\n",
      "        25117, 25191, 26215, 26299, 26387, 27411, 28435, 29039, 30063, 31087,\n",
      "        32111])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  109\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  110\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  111\n",
      "  Batch 3800: tensor([    0,  1024,  1110,  2134,  3158,  3284,  4308,  5332,  5415,  6439,\n",
      "         7030,  7116,  7205,  8229,  8311,  9021,  9123, 10147, 11171, 11249,\n",
      "        12273, 12358, 13137, 13853, 14877, 14971, 15055, 16079, 17103, 17276,\n",
      "        17354, 18378, 19402, 19485, 19579, 20603, 20686, 21710, 21801, 22825,\n",
      "        23849, 24873, 25897, 26178, 26268, 27100, 27289, 28313, 28397, 29421,\n",
      "        30445, 30733, 31095, 32119])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  112\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  113\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  114\n",
      "  Batch 3900: tensor([    0,  1024,  2048,  2128,  3152,  4176,  4258,  5282,  6306,  7330,\n",
      "         8354,  9167, 10191, 10276, 10354, 11378, 11614, 11699, 12723, 13747,\n",
      "        14771, 15795, 16819, 17099, 18123, 18211, 19235, 19319, 19394, 20418,\n",
      "        21170, 22194, 22294, 22380, 23404, 24416, 25440, 26427, 26506, 26593,\n",
      "        27617, 28641, 29665, 29757, 30781, 30861, 30949, 31973])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  115\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  116\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  117\n",
      "  Batch 4000: tensor([    0,  1024,  1110,  1459,  1541,  2565,  3589,  3683,  4707,  5731,\n",
      "         6755,  7779,  8766,  9790,  9893, 10917, 11007, 11091, 12115, 12201,\n",
      "        12314, 13338, 13428, 13521, 14545, 15569, 16593, 17617, 17703, 18727,\n",
      "        19751, 20775, 21799, 21878, 22006, 22715, 22806, 23830, 23918, 24942,\n",
      "        25966, 26049, 27073, 28097, 29121, 30145, 31169, 32091, 32177])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  118\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  119\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  120\n",
      "  Batch 4100: tensor([    0,  1024,  1102,  1393,  1488,  2345,  3369,  4393,  5417,  5500,\n",
      "         5582,  6606,  7630,  7717,  7797,  7898,  8510,  9534,  9608,  9696,\n",
      "         9803, 10827, 11851, 12875, 13847, 14871, 15744, 16768, 17792, 18816,\n",
      "        19840, 19936, 20049, 20143, 21007, 22031, 23055, 23985, 25009, 25092,\n",
      "        26116, 27140, 27231, 28202, 29226, 30250, 30327, 31351, 32375])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  121\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  122\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  123\n",
      "  Batch 4200: tensor([    0,  1024,  2048,  3072,  3179,  4203,  5227,  5318,  6342,  7366,\n",
      "         8390,  8468,  8550,  9492, 10516, 11540, 12080, 13104, 14128, 15152,\n",
      "        16176, 17072, 17178, 17263, 18287, 19311, 20242, 21266, 22290, 23314,\n",
      "        23921, 24016, 24099, 25123, 25203, 25279, 25363, 25501, 26525, 26851,\n",
      "        26953, 27977, 29001, 30025, 31049, 31133, 32157])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  124\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  125\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  126\n",
      "  Batch 4300: tensor([    0,  1024,  2048,  2635,  3659,  3799,  4823,  5847,  6871,  7766,\n",
      "         8790,  9252, 10276, 11300, 12324, 13348, 13437, 13522, 14546, 15095,\n",
      "        16119, 17143, 17559, 17640, 18236, 19145, 19268, 19350, 20374, 21398,\n",
      "        22422, 22510, 22590, 23233, 24257, 25281, 25466, 26490, 27514, 27596,\n",
      "        28620, 28697, 29721, 30745, 30900, 30979, 32003])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  127\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  128\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  129\n",
      "  Batch 4400: tensor([    0,  1024,  2048,  3072,  3156,  3768,  4792,  4870,  5894,  6918,\n",
      "         7942,  8034,  9058, 10082, 11106, 11858, 12882, 13762, 14786, 14872,\n",
      "        15896, 16856, 17880, 18730, 19643, 20667, 21691, 22715, 23739, 23825,\n",
      "        24476, 24565, 24651, 24730, 25316, 26340, 26984, 28008, 29032, 30056,\n",
      "        31080, 31167, 32031, 32128, 32213])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  130\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  131\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  132\n",
      "  Batch 4500: tensor([    0,  1024,  2048,  2245,  3269,  4293,  4894,  5918,  6942,  7966,\n",
      "         8338,  9362, 10386, 11278, 11358, 12382, 13406, 13494, 14518, 15542,\n",
      "        15894, 15979, 16059, 16917, 17555, 18579, 18659, 19683, 19972, 20996,\n",
      "        21083, 21861, 22885, 23601, 24625, 24766, 24937, 25019, 26043, 27067,\n",
      "        27157, 27255, 27344, 28368, 28450, 28536, 28619, 29643, 30667, 31691,\n",
      "        31779, 31867])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  133\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  134\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  135\n",
      "  Batch 4600: tensor([    0,  1024,  1111,  1204,  1299,  2323,  2412,  2503,  3527,  4551,\n",
      "         5575,  6599,  6687,  7711,  8735,  9759, 10783, 11807, 12185, 13209,\n",
      "        13299, 13385, 13466, 14490, 15514, 16516, 17540, 18564, 19588, 19676,\n",
      "        19757, 20781, 21805, 22829, 23845, 23927, 24011, 25035, 25117, 26141,\n",
      "        26240, 27264, 27346, 28370, 29394, 30418, 30496, 31520, 31650, 32674])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  136\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  137\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  138\n",
      "  Batch 4700: tensor([    0,  1024,  2048,  2126,  3138,  4162,  4249,  5273,  6297,  7321,\n",
      "         8345,  8428,  9452, 10476, 11500, 11765, 12789, 12867, 13891, 13978,\n",
      "        15002, 15075, 16099, 16181, 17205, 17298, 17385, 17893, 18876, 18956,\n",
      "        19980, 21004, 22028, 23052, 24076, 24161, 24458, 24655, 24742, 25029,\n",
      "        25125, 25601, 25950, 26037, 27061, 28085, 28173, 28769, 29793, 30817,\n",
      "        31841, 32147, 33171])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  139\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  140\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  141\n",
      "  Batch 4800: tensor([    0,  1024,  1106,  1194,  1280,  2304,  2487,  3511,  4378,  5252,\n",
      "         6276,  7300,  8324,  8394,  8487,  8573,  8758,  8848,  9872, 10896,\n",
      "        11920, 12944, 13952, 14554, 15578, 15659, 15742, 16766, 16894, 17918,\n",
      "        18942, 19966, 20112, 20241, 20325, 21136, 22160, 22248, 23272, 24296,\n",
      "        25320, 26344, 27368, 28392, 29416, 29503, 30527, 31551, 32429, 32523,\n",
      "        32608])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  142\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  143\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  144\n",
      "  Batch 4900: tensor([    0,  1024,  2048,  2129,  2238,  3206,  4230,  4389,  4477,  4564,\n",
      "         4651,  5675,  6699,  6777,  7391,  7478,  7569,  8593,  8672,  8963,\n",
      "         9201, 10225, 11249, 12273, 12362, 13386, 14410, 15434, 16458, 17482,\n",
      "        18506, 19530, 19727, 20751, 21775, 22799, 23823, 24847, 25871, 26895,\n",
      "        26972, 27996, 28072, 29096, 30120, 31116, 31202, 31288, 32312])\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  145\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  146\n",
      "End of dataloader, restarting...\n",
      "We are now at epoch:  147\n",
      "  Batch 5000: tensor([    0,  1024,  1143,  1232,  2256,  2350,  2424,  3448,  3529,  3901,\n",
      "         4925,  5004,  5779,  6803,  7827,  8851,  8927,  9951, 10975, 11983,\n",
      "        13007, 14031, 15055, 15134, 16158, 17182, 17270, 18294, 18371, 19395,\n",
      "        20419, 21443, 22467, 22885, 23668, 24692, 25587, 25784, 26740, 27764,\n",
      "        28788, 29620, 29898, 29984, 31008, 31104, 32128, 32213])\n",
      "\n",
      "Total batches generated: 5000\n",
      "Average batch time: 0.07s\n",
      "Min/Max batch time: 0.00s / 2.68s\n"
     ]
    }
   ],
   "source": [
    "import axolotl.data_nested as data\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "DATASET_TRAIN = \"/mnt/e/datasets/IPR036736_90_grouped/train\"\n",
    "DATASET_TEST = \"/mnt/e/datasets/IPR036736_90_grouped/test\"\n",
    "\n",
    "train_loader, test_loader = data.get_dataloaders(\n",
    "    train_batch_size=BATCH_SIZE,\n",
    "    valid_batch_size=BATCH_SIZE,\n",
    "    ngpus=NUM_GPUS,\n",
    "    accum=1,\n",
    "    train_path=DATASET_TRAIN,\n",
    "    valid_path=DATASET_TEST,\n",
    "    max_length=CTX_LEN,\n",
    "    drop_last=True,\n",
    "    num_workers=4,\n",
    "    distributed=False,\n",
    "    seed=42, # Seed for picking the data in the clusters and making torch generators.\n",
    "    epoch=0,\n",
    "    shuffle_each_epoch=True,\n",
    ")\n",
    "\n",
    "train_iter = iter(train_loader)\n",
    "test_iter = iter(test_loader)\n",
    "\n",
    "# Test first batch generation\n",
    "print(\"Testing first batch generation:\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    first_train_batch = next(train_iter)\n",
    "    first_train_time = time.time() - start_time\n",
    "    print(f\"  First train batch: {first_train_time:.2f}s\")\n",
    "    print(f\"  First train batch tokens: {first_train_batch['input_ids'].offsets()}\")\n",
    "except:\n",
    "    print(\"  First train batch: No batches generated\")\n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    first_test_batch = next(test_iter)\n",
    "    first_test_time = time.time() - start_time\n",
    "    print(f\"  First test batch: {first_test_time:.2f}s\")\n",
    "    print(f\"  First train batch tokens: {first_test_batch['input_ids'].offsets()}\")\n",
    "except:\n",
    "    print(\"  First test batch: No batches generated\")\n",
    "\n",
    "# Test subsequent batch generation\n",
    "print(\"\\nTesting subsequent batch generation:\")\n",
    "batch_count = 0\n",
    "times = []\n",
    "while True:\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        batch = next(train_iter)\n",
    "        batch_time = time.time() - start_time\n",
    "        times.append(batch_time)\n",
    "        batch_count += 1\n",
    "        if batch_count % 100 == 0:  # Print every 10 batches\n",
    "            print(f\"  Batch {batch_count}: {batch['input_ids'].offsets()}\")\n",
    "    except StopIteration:\n",
    "        print(\"  No more batches available\")\n",
    "        break\n",
    "\n",
    "    if batch_count >= 5000:  # Limit to 50 batches\n",
    "        break\n",
    "\n",
    "print(f\"\\nTotal batches generated: {batch_count}\")\n",
    "if times:\n",
    "    print(f\"Average batch time: {np.mean(times):.2f}s\")\n",
    "    print(f\"Min/Max batch time: {np.min(times):.2f}s / {np.max(times):.2f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
