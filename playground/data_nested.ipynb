{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kkj/axolotl/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk, Dataset\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader, DistributedSampler, Sampler\n",
    "from typing import Optional, List\n",
    "\n",
    "def get_dataset(name):\n",
    "    dataset = load_from_disk(name)\n",
    "    dataset = dataset.with_format('torch')\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def cycle_loader(dataloader):\n",
    "    while True:\n",
    "        for data in dataloader:\n",
    "            yield data\n",
    "        # go to next epoch\n",
    "        dataloader.batch_sampler.set_epoch(dataloader.batch_sampler.epoch + 1)\n",
    "\n",
    "class SequencePackingSampler(Sampler):\n",
    "    def __init__(self, \n",
    "                 dataset,\n",
    "                 indices: List[int]=None,\n",
    "                 max_length: int=None, \n",
    "                 total_length: int=None,\n",
    "                 seed=0,\n",
    "                 drop_last=False,\n",
    "    ):\n",
    "        \n",
    "        self.dataset = dataset # a clustered dataset\n",
    "        self.max_length = max_length\n",
    "        self.total_length = total_length\n",
    "        self.seed = seed\n",
    "        self.epoch = 0\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "        if indices is None: # Indices should only be used with ditributed sampler\n",
    "            self.indices = list(range(len(dataset)))\n",
    "            self.shuffle = True # only shuffle when we are not using distributed sampler. The distributed sampler should handle the shuffling\n",
    "        else:\n",
    "            self.indices = indices\n",
    "            self.shuffle = False # we only need to shuffle when we are not using distributed sampler. The distributed sampler should handle the shuffling\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            g = torch.Generator().manual_seed(self.seed + self.epoch)\n",
    "            indices = torch.randperm(len(self.indices), generator=g).tolist()\n",
    "        else:\n",
    "            indices = self.indices\n",
    "\n",
    "        batch = []\n",
    "        batch_length = 0\n",
    "        for idx in indices:\n",
    "            cluster_size = self.dataset[idx]['cluster_size'].item()\n",
    "            if cluster_size == 1:\n",
    "                cluster_idx = 0\n",
    "            else:\n",
    "                # To deterministically sample from the clusters, we use the epoch to index into the cluster, same as in the collate_fn (very important)\n",
    "                cluster_idx = torch.randint(0, cluster_size, (1,), generator=torch.Generator().manual_seed(self.seed + self.epoch)).item()\n",
    "\n",
    "            length = self.dataset[idx]['length'][cluster_idx].item()\n",
    "            length = min(length, self.max_length)\n",
    "            \n",
    "            batch.append(idx)\n",
    "            batch_length += length\n",
    "\n",
    "            if batch_length >= self.total_length:\n",
    "                yield batch\n",
    "                batch = []\n",
    "                batch_length = 0\n",
    "\n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yield batch\n",
    "\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        g = torch.Generator().manual_seed(self.seed + self.epoch)\n",
    "\n",
    "        # get the sizes of the clusters\n",
    "        cluster_sizes = [x['cluster_size'] for x in batch]\n",
    "        \n",
    "        indexes = []\n",
    "        for cs in cluster_sizes:\n",
    "            if cs == 1:\n",
    "                indexes.append(0)\n",
    "                continue\n",
    "            idx = torch.randint(0, cs, (1,), generator=torch.Generator().manual_seed(self.seed + self.epoch)).item()\n",
    "            indexes.append(idx)\n",
    "\n",
    "        # get the input_ids for each cluster\n",
    "        input_ids_list = [x[\"input_ids\"][i] for x, i in zip(batch, indexes)]\n",
    "        input_ids_list = [maybe_truncate(input_ids, self.max_length, generator=g) for input_ids in input_ids_list[:-1]]\n",
    "\n",
    "        # the last input ids should be truncated to the remaining length, to not exceed the total length\n",
    "        length_sum = sum([min(x[\"length\"][i].item(), self.max_length) for x, i in zip(batch, indexes[:-1])])\n",
    "        last_length = min(self.total_length - length_sum, self.max_length)\n",
    "        # print(\"total length: \", length_sum + last_length)\n",
    "\n",
    "        input_ids_list.append(maybe_truncate(input_ids_list[-1], last_length, generator=g))\n",
    "\n",
    "        # convert to nested tensor for the model\n",
    "        input_ids = torch.nested.nested_tensor(input_ids_list, layout=torch.jagged)\n",
    "        label = torch.tensor([x[\"label\"][i] for x, i in zip(batch, indexes)])\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"label\": label}\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError(\"SequencePackingSampler does not support __len__\")\n",
    "    \n",
    "    def set_epoch(self, epoch: int) -> None:\n",
    "        r\"\"\"\n",
    "        Set the epoch for this sampler.\n",
    "\n",
    "        When :attr:`shuffle=True`, this ensures all replicas\n",
    "        use a different random ordering for each epoch. Otherwise, the next iteration of this\n",
    "        sampler will yield the same ordering.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): Epoch number.\n",
    "        \"\"\"\n",
    "        self.epoch = epoch\n",
    "\n",
    "\n",
    "class DistributedSequencePackingSampler(DistributedSampler):\n",
    "    def __init__(self, \n",
    "                 dataset, \n",
    "                 num_replicas=None, \n",
    "                 rank=None, \n",
    "                 shuffle=True, \n",
    "                 max_length: int=None, # max length of each sequence in the batch\n",
    "                 total_length: int=None, # total length of the batch (total amount of tokens)\n",
    "                 seed=0,\n",
    "                 drop_last=False,\n",
    "    ):\n",
    "        super().__init__(dataset, num_replicas=num_replicas, rank=rank, shuffle=shuffle, seed=seed, drop_last=False)\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.total_length = total_length\n",
    "        self.seed = seed\n",
    "        self.dataset = dataset\n",
    "        self.drop_last = drop_last\n",
    "        self.epoch = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.indices = list(super().__iter__())\n",
    "        batch_sampler = SequencePackingSampler(self.dataset, \n",
    "                                               indices=self.indices, \n",
    "                                               max_length=self.max_length, \n",
    "                                               total_length=self.total_length, \n",
    "                                               seed=self.seed,\n",
    "                                               drop_last=self.drop_last,\n",
    "        )\n",
    "        batch_sampler.set_epoch(self.epoch) # set the epoch to the epoch of the distributed sampler\n",
    "        self.collate_fn = batch_sampler.collate_fn # set the collate_fn to the collate_fn of the SequencePackingSampler\n",
    "        return iter(batch_sampler)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        raise NotImplementedError(\"DistributedSequencePackingSampler does not support collate_fn. It should be updated in __iter__ by the SequencePackingSampler\")\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError(\"DistributedSequencePackingSampler does not support __len__\")\n",
    "\n",
    "    def set_epoch(self, epoch: int) -> None:\n",
    "        r\"\"\"\n",
    "        Set the epoch for this sampler.\n",
    "\n",
    "        When :attr:`shuffle=True`, this ensures all replicas\n",
    "        use a different random ordering for each epoch. Otherwise, the next iteration of this\n",
    "        sampler will yield the same ordering.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): Epoch number.\n",
    "        \"\"\"\n",
    "        self.epoch = epoch\n",
    "\n",
    "def maybe_truncate(input_ids, max_len, generator=None):\n",
    "    # Truncate the input_ids if it is longer than max_len\n",
    "    seq_len = len(input_ids)\n",
    "    if seq_len > max_len:\n",
    "        index = torch.randint(0, seq_len - max_len, (1,), generator=generator).item()\n",
    "        input_ids = input_ids[index:index + max_len]\n",
    "    return input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Default process group has not been initialized, please make sure to call init_process_group.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m train_set \u001b[38;5;241m=\u001b[39m get_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/kkj/axolotl/datasets/90_IPR036736_grouped/train\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m valid_set \u001b[38;5;241m=\u001b[39m get_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/kkj/axolotl/datasets/90_IPR036736_grouped/valid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m train_sampler \u001b[38;5;241m=\u001b[39m \u001b[43mDistributedSequencePackingSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# TODO: make sure this gets the right length with distributed, and make it distribute properly\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mtotal_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# TODO: make sure this gets the right length with distributed, and make it distribute properly\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m val_sampler \u001b[38;5;241m=\u001b[39m DistributedSequencePackingSampler(train_set,\n\u001b[1;32m     10\u001b[0m                                                     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, \u001b[38;5;66;03m# TODO: make sure this gets the right length with distributed, and make it distribute properly\u001b[39;00m\n\u001b[1;32m     11\u001b[0m                                                     total_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;66;03m# TODO: make sure this gets the right length with distributed, and make it distribute properly\u001b[39;00m\n\u001b[1;32m     12\u001b[0m                                                     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m cycle_loader(DataLoader(\n\u001b[1;32m     16\u001b[0m     train_set,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# batch_size=config.training.batch_size // (config.ngpus * config.training.accum),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     persistent_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     24\u001b[0m ))\n",
      "Cell \u001b[0;32mIn[1], line 135\u001b[0m, in \u001b[0;36mDistributedSequencePackingSampler.__init__\u001b[0;34m(self, dataset, num_replicas, rank, shuffle, max_length, total_length, seed, drop_last)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    126\u001b[0m              dataset, \n\u001b[1;32m    127\u001b[0m              num_replicas\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m              drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    134\u001b[0m ):\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_replicas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_replicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length \u001b[38;5;241m=\u001b[39m max_length\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_length \u001b[38;5;241m=\u001b[39m total_length\n",
      "File \u001b[0;32m~/axolotl/.venv/lib/python3.10/site-packages/torch/utils/data/distributed.py:78\u001b[0m, in \u001b[0;36mDistributedSampler.__init__\u001b[0;34m(self, dataset, num_replicas, rank, shuffle, seed, drop_last)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dist\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequires distributed package to be available\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 78\u001b[0m     num_replicas \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_world_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dist\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[0;32m~/axolotl/.venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2331\u001b[0m, in \u001b[0;36mget_world_size\u001b[0;34m(group)\u001b[0m\n\u001b[1;32m   2328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _rank_not_in_group(group):\n\u001b[1;32m   2329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_group_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/axolotl/.venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:1098\u001b[0m, in \u001b[0;36m_get_group_size\u001b[0;34m(group)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get a given group's world size.\"\"\"\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m GroupMember\u001b[38;5;241m.\u001b[39mWORLD \u001b[38;5;129;01mor\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1098\u001b[0m     default_pg \u001b[38;5;241m=\u001b[39m \u001b[43m_get_default_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_pg\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m group\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[0;32m~/axolotl/.venv/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:1304\u001b[0m, in \u001b[0;36m_get_default_group\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the default process group created by init_process_group.\"\"\"\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_initialized():\n\u001b[0;32m-> 1304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefault process group has not been initialized, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease make sure to call init_process_group.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1307\u001b[0m     )\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m not_none(GroupMember\u001b[38;5;241m.\u001b[39mWORLD)\n",
      "\u001b[0;31mValueError\u001b[0m: Default process group has not been initialized, please make sure to call init_process_group."
     ]
    }
   ],
   "source": [
    "\n",
    "train_set = get_dataset('/home/kkj/axolotl/datasets/90_IPR036736_grouped/train')\n",
    "valid_set = get_dataset('/home/kkj/axolotl/datasets/90_IPR036736_grouped/valid')\n",
    "\n",
    "train_sampler = DistributedSequencePackingSampler(train_set,\n",
    "                                                    max_length=1024, # TODO: make sure this gets the right length with distributed, and make it distribute properly\n",
    "                                                    total_length=1024 * 4 // (1 * 1), # TODO: make sure this gets the right length with distributed, and make it distribute properly\n",
    "                                                    drop_last=True,\n",
    ")\n",
    "val_sampler = DistributedSequencePackingSampler(train_set,\n",
    "                                                    max_length=1024, # TODO: make sure this gets the right length with distributed, and make it distribute properly\n",
    "                                                    total_length=1024 * 4 // (1 * 1), # TODO: make sure this gets the right length with distributed, and make it distribute properly\n",
    "                                                    drop_last=True,\n",
    ")\n",
    "\n",
    "train_loader = cycle_loader(DataLoader(\n",
    "    train_set,\n",
    "    # batch_size=config.training.batch_size // (config.ngpus * config.training.accum),\n",
    "    batch_sampler=train_sampler,\n",
    "    num_workers=8,\n",
    "    collate_fn=train_sampler.collate_fn,\n",
    "    pin_memory=True,\n",
    "    shuffle=(train_sampler is None),\n",
    "    persistent_workers=True,\n",
    "))\n",
    "\n",
    "val_loader = cycle_loader(DataLoader(\n",
    "    valid_set,\n",
    "    # batch_size=config.training.batch_size // (config.ngpus * config.training.accum),\n",
    "    batch_sampler=val_sampler,\n",
    "    num_workers=8,\n",
    "    collate_fn=val_sampler.collate_fn,\n",
    "    pin_memory=True,\n",
    "    shuffle=(val_sampler is None),\n",
    "    persistent_workers=True,\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
