{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t12_35M_UR50D.pt\" to /home/kkj/.cache/torch/hub/checkpoints/esm2_t12_35M_UR50D.pt\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t12_35M_UR50D-contact-regression.pt\" to /home/kkj/.cache/torch/hub/checkpoints/esm2_t12_35M_UR50D-contact-regression.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<cls>',\n",
       " '<pad>',\n",
       " '<eos>',\n",
       " '<unk>',\n",
       " 'L',\n",
       " 'A',\n",
       " 'G',\n",
       " 'V',\n",
       " 'S',\n",
       " 'E',\n",
       " 'R',\n",
       " 'T',\n",
       " 'I',\n",
       " 'D',\n",
       " 'P',\n",
       " 'K',\n",
       " 'Q',\n",
       " 'N',\n",
       " 'F',\n",
       " 'Y',\n",
       " 'M',\n",
       " 'H',\n",
       " 'W',\n",
       " 'C',\n",
       " 'X',\n",
       " 'B',\n",
       " 'U',\n",
       " 'Z',\n",
       " 'O',\n",
       " '.',\n",
       " '-',\n",
       " '<null_1>',\n",
       " '<mask>']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model, alphabet = esm.pretrained.esm2_t12_35M_UR50D()\n",
    "eval_model = eval_model.to(\"cuda\")\n",
    "eval_model = eval_model.eval()\n",
    "alphabet.all_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 67])\n",
      "['protein1', 'protein2']\n",
      "['MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG', 'KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGI']\n",
      "tensor([[ 0, 20, 15, 11,  7, 10, 16,  9, 10,  4, 15,  8, 12,  7, 10, 12,  4,  9,\n",
      "         10,  8, 15,  9, 14,  7,  8,  6,  5, 16,  4,  5,  9,  9,  4,  8,  7,  8,\n",
      "         10, 16,  7, 12,  7, 16, 13, 12,  5, 19,  4, 10,  8,  4,  6, 19, 17, 12,\n",
      "          7,  5, 11, 14, 10,  6, 19,  7,  4,  5,  6,  6,  2],\n",
      "        [ 0, 15,  5,  4, 11,  5, 10, 16, 16,  9,  7, 18, 13,  4, 12, 10, 13, 21,\n",
      "         12,  8, 16, 11,  6, 20, 14, 14, 11, 10,  5,  9, 12,  5, 16, 10,  4,  6,\n",
      "         18, 10,  8, 14, 17,  5,  5,  9,  9, 21,  4, 15,  5,  4,  5, 10, 15,  6,\n",
      "          7, 12,  9, 12,  7,  8,  6,  5,  8, 10,  6, 12,  2]])\n",
      "[0, 20, 15, 11, 7, 10, 16, 9, 10, 4, 15, 8, 12, 7, 10, 12, 4, 9, 10, 8, 15, 9, 14, 7, 8, 6, 5, 16, 4, 5, 9, 9, 4, 8, 7, 8, 10, 16, 7, 12, 7, 16, 13, 12, 5, 19, 4, 10, 8, 4, 6, 19, 17, 12, 7, 5, 11, 14, 10, 6, 19, 7, 4, 5, 6, 6]\n"
     ]
    }
   ],
   "source": [
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "data = [\n",
    "    (\"protein1\", \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"),\n",
    "    (\"protein2\", \"KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGI\"),\n",
    "]\n",
    "\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "print(batch_tokens.shape)\n",
    "print(batch_labels)\n",
    "print(batch_strs)\n",
    "print(batch_tokens)\n",
    "print(alphabet.encode(\"<cls>MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<cls> 21\n",
      "<pad> 20\n",
      "<eos> 22\n",
      "<unk> 20\n",
      "L 9\n",
      "A 0\n",
      "G 5\n",
      "V 17\n",
      "S 15\n",
      "E 3\n",
      "R 14\n",
      "T 16\n",
      "I 7\n",
      "D 2\n",
      "P 12\n",
      "K 8\n",
      "Q 13\n",
      "N 11\n",
      "F 4\n",
      "Y 19\n",
      "M 10\n",
      "H 6\n",
      "W 18\n",
      "C 1\n",
      "X 20\n",
      "B 20\n",
      "U 20\n",
      "Z 20\n",
      "O 20\n",
      ". 20\n",
      "- 20\n",
      "<null_1> 20\n",
      "<mask> 23\n",
      "[[5], [23], [13], [9], [18], [6], [21], [12], [15], [4], [20], [17], [14], [16], [10], [8], [11], [7], [22], [19], [1, 3, 24, 25, 26, 27, 28, 29, 30, 31], [0], [2], [32]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kkj/axolotl/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## The absorb mapping\n",
    "# \"A\": 0,\n",
    "# \"C\": 1,\n",
    "# \"D\": 2,\n",
    "# \"E\": 3,\n",
    "# \"F\": 4,\n",
    "# \"G\": 5,\n",
    "# \"H\": 6,\n",
    "# \"I\": 7,\n",
    "# \"K\": 8,\n",
    "# \"L\": 9,\n",
    "# \"M\": 10,\n",
    "# \"N\": 11,\n",
    "# \"P\": 12,\n",
    "# \"Q\": 13,\n",
    "# \"R\": 14,\n",
    "# \"S\": 15,\n",
    "# \"T\": 16,\n",
    "# \"V\": 17,\n",
    "# \"W\": 18,\n",
    "# \"Y\": 19,\n",
    "# \"?\": 20,\n",
    "# \"[\": 21,\n",
    "# \"]\": 22,\n",
    "# \"-\": 23\n",
    "\n",
    "## The esm mapping\n",
    "# ['<cls>',\n",
    "#  '<pad>',\n",
    "#  '<eos>',\n",
    "#  '<unk>',\n",
    "#  'L',\n",
    "#  'A',\n",
    "#  'G',\n",
    "#  'V',\n",
    "#  'S',\n",
    "#  'E',\n",
    "#  'R',\n",
    "#  'T',\n",
    "#  'I',\n",
    "#  'D',\n",
    "#  'P',\n",
    "#  'K',\n",
    "#  'Q',\n",
    "#  'N',\n",
    "#  'F',\n",
    "#  'Y',\n",
    "#  'M',\n",
    "#  'H',\n",
    "#  'W',\n",
    "#  'C',\n",
    "#  'X',\n",
    "#  'B',\n",
    "#  'U',\n",
    "#  'Z',\n",
    "#  'O',\n",
    "#  '.',\n",
    "#  '-',\n",
    "#  '<null_1>',\n",
    "#  '<mask>']\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"/home/kkj/axolotl/tokenizer/tokenizer_absorb\")\n",
    "\n",
    "esm_token_mapping = [[] for _ in range(tokenizer.vocab_size)]\n",
    "\n",
    "for i, tok in enumerate(alphabet.all_toks):\n",
    "    if tok == '<cls>':\n",
    "        in_tok = '['\n",
    "    elif tok == '<eos>':\n",
    "        in_tok = ']'\n",
    "    elif tok == '<unk>':\n",
    "        in_tok = '?'\n",
    "    elif tok == '<mask>':\n",
    "        in_tok = '-'\n",
    "    elif tok == '-': # '-' is what i use as mask token\n",
    "        in_tok = '?'\n",
    "    else:\n",
    "        in_tok = tok\n",
    "\n",
    "    tok_id = tokenizer.encode(in_tok)[1]\n",
    "    print(tok, tok_id)\n",
    "    esm_token_mapping[tok_id].append(i)\n",
    "\n",
    "print(esm_token_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'eval_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     batch_tokens_masked \u001b[38;5;241m=\u001b[39m batch_tokens_masked\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 11\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[43meval_model\u001b[49m(batch_tokens_masked)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     12\u001b[0m     esm_logits[:, i, :] \u001b[38;5;241m=\u001b[39m logits[:, i, :]\n\u001b[1;32m     14\u001b[0m new_esm_logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m67\u001b[39m, \u001b[38;5;241m23\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_model' is not defined"
     ]
    }
   ],
   "source": [
    "model_logits = torch.randn(2, 67, 23)\n",
    "esm_logits = torch.zeros(2, 67, 33)\n",
    "\n",
    "batch_size = batch_tokens.shape[0]\n",
    "for i in range(batch_tokens.shape[-1]):\n",
    "    batch_tokens_masked = batch_tokens.clone()\n",
    "    batch_tokens_masked[:, i] = alphabet.mask_idx\n",
    "    batch_tokens_masked = batch_tokens_masked.to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        logits = eval_model(batch_tokens_masked)[\"logits\"]\n",
    "    esm_logits[:, i, :] = logits[:, i, :]\n",
    "\n",
    "new_esm_logits = torch.zeros(2, 67, 23)\n",
    "for i in range(model_logits.shape[-1]):\n",
    "    new_esm_logits[:, :, i] = esm_logits[:, :, esm_token_mapping[i]].sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(120.4476)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "perplexity = F.cross_entropy(new_esm_logits, model_logits, reduction=\"none\").mean(dim=-1).exp().mean()\n",
    "print(perplexity)\n",
    "\n",
    "del eval_model, logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
