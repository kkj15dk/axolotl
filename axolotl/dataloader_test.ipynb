{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_nested as data\n",
    "import torch\n",
    "\n",
    "train_dl, eval_dl = data.get_dataloaders(128,\n",
    "                                            128,\n",
    "                                            1,\n",
    "                                            1,\n",
    "                                            '/home/kkj/axolotl/datasets/IPR036736_90_grouped/train',\n",
    "                                            '/home/kkj/axolotl/datasets/IPR036736_90_grouped/valid',\n",
    "                                            1024,\n",
    "                                            False,\n",
    "                                            4, # num_workers\n",
    "                                            distributed=True,\n",
    "                                            shuffle_each_epoch=True,\n",
    ")\n",
    "\n",
    "train_iter = iter(train_dl)\n",
    "eval_iter = iter(eval_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train')\n",
    "for i, batch in enumerate(eval_iter):\n",
    "    seq_lens: torch.Tensor = batch['input_ids'].offsets().diff()\n",
    "    if i % 100 == 0:\n",
    "        print(i, seq_lens.sum())\n",
    "    \n",
    "    if seq_lens.all():\n",
    "        pass\n",
    "    else:\n",
    "        print(i, seq_lens)\n",
    "        print(seq_lens.sum())\n",
    "        raise ValueError('seq_lens not all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Distributed DataLoader Setup\n",
      "==================================================\n",
      "\n",
      "--- Testing Rank 0/1 ---\n",
      "  Batch 0: 97 sequences, 65536 tokens\n",
      "  Batch 0: 97 sequences, 65536 tokens\n",
      "  Batch 1: 86 sequences, 65536 tokens\n",
      "  Batch 1: 86 sequences, 65536 tokens\n",
      "  Batch 2: 102 sequences, 65536 tokens\n",
      "  Batch 2: 102 sequences, 65536 tokens\n",
      "  Batch 3: 94 sequences, 65536 tokens\n",
      "  Batch 3: 94 sequences, 65536 tokens\n",
      "  Batch 4: 97 sequences, 65536 tokens\n",
      "  Batch 4: 97 sequences, 65536 tokens\n",
      "  Batch 5: 95 sequences, 65536 tokens\n",
      "  Batch 5: 95 sequences, 65536 tokens\n",
      "  Batch 6: 101 sequences, 65536 tokens\n",
      "  Batch 6: 101 sequences, 65536 tokens\n",
      "  Batch 7: 100 sequences, 65536 tokens\n",
      "  Batch 7: 100 sequences, 65536 tokens\n",
      "  Batch 8: 81 sequences, 53199 tokens\n",
      "  Batch 8: 81 sequences, 53199 tokens\n",
      "  Batch 9: 106 sequences, 65536 tokens\n",
      "  Batch 9: 106 sequences, 65536 tokens\n",
      "  Rank 0 processed 10 batches, 643023 total tokens\n",
      "\n",
      "--- Testing Rank 1/1 ---\n",
      "  Rank 0 processed 10 batches, 643023 total tokens\n",
      "\n",
      "--- Testing Rank 1/1 ---\n",
      "  Batch 0: 91 sequences, 65536 tokens\n",
      "  Batch 0: 91 sequences, 65536 tokens\n",
      "  Batch 1: 105 sequences, 65536 tokens\n",
      "  Batch 1: 105 sequences, 65536 tokens\n",
      "  Batch 2: 100 sequences, 65536 tokens\n",
      "  Batch 2: 100 sequences, 65536 tokens\n",
      "  Batch 3: 105 sequences, 65536 tokens\n",
      "  Batch 3: 105 sequences, 65536 tokens\n",
      "  Batch 4: 100 sequences, 65536 tokens\n",
      "  Batch 4: 100 sequences, 65536 tokens\n",
      "  Batch 5: 95 sequences, 65536 tokens\n",
      "  Batch 5: 95 sequences, 65536 tokens\n",
      "  Batch 6: 96 sequences, 65536 tokens\n",
      "  Batch 6: 96 sequences, 65536 tokens\n",
      "  Batch 7: 105 sequences, 65536 tokens\n",
      "  Batch 7: 105 sequences, 65536 tokens\n",
      "  Batch 8: 96 sequences, 65536 tokens\n",
      "  Batch 8: 96 sequences, 65536 tokens\n",
      "  Batch 9: 101 sequences, 65536 tokens\n",
      "  Batch 9: 101 sequences, 65536 tokens\n",
      "  Rank 1 processed 10 batches, 655360 total tokens\n",
      "\n",
      "--- Distribution Analysis ---\n",
      "World size: 2\n",
      "Rank 0: 10 batches, 643023 tokens\n",
      "Rank 1: 10 batches, 655360 tokens\n",
      "\n",
      "Rank 0 token counts: [65536, 65536, 65536, 65536, 65536]...\n",
      "Rank 1 token counts: [65536, 65536, 65536, 65536, 65536]...\n",
      "✓ Ranks are getting different batches (good!)\n",
      "\n",
      "--- Testing Epoch Cycling ---\n",
      "  Rank 1 processed 10 batches, 655360 total tokens\n",
      "\n",
      "--- Distribution Analysis ---\n",
      "World size: 2\n",
      "Rank 0: 10 batches, 643023 tokens\n",
      "Rank 1: 10 batches, 655360 tokens\n",
      "\n",
      "Rank 0 token counts: [65536, 65536, 65536, 65536, 65536]...\n",
      "Rank 1 token counts: [65536, 65536, 65536, 65536, 65536]...\n",
      "✓ Ranks are getting different batches (good!)\n",
      "\n",
      "--- Testing Epoch Cycling ---\n",
      "Step 0: Epoch ?, 56 sequences, 32768 tokens\n",
      "Step 0: Epoch ?, 56 sequences, 32768 tokens\n",
      "Step 1: Epoch ?, 46 sequences, 32768 tokens\n",
      "Step 1: Epoch ?, 46 sequences, 32768 tokens\n",
      "Step 2: Epoch ?, 53 sequences, 32768 tokens\n",
      "Step 2: Epoch ?, 53 sequences, 32768 tokens\n",
      "Step 3: Epoch ?, 56 sequences, 32768 tokens\n",
      "Step 3: Epoch ?, 56 sequences, 32768 tokens\n",
      "Step 4: Epoch ?, 63 sequences, 32768 tokens\n",
      "\n",
      "Distributed testing complete!\n",
      "Step 4: Epoch ?, 63 sequences, 32768 tokens\n",
      "\n",
      "Distributed testing complete!\n"
     ]
    }
   ],
   "source": [
    "# Test distributed setup by mocking the distributed environment\n",
    "import torch.distributed as dist\n",
    "from unittest.mock import patch\n",
    "import os\n",
    "\n",
    "import data_nested as data\n",
    "\n",
    "print(\"Testing Distributed DataLoader Setup\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Mock distributed environment for testing\n",
    "def mock_distributed_setup(world_size=2, rank=0):\n",
    "    \"\"\"Mock distributed environment for testing purposes\"\"\"\n",
    "    \n",
    "    # Patch the distributed functions\n",
    "    def mock_get_world_size():\n",
    "        return world_size\n",
    "    \n",
    "    def mock_get_rank():\n",
    "        return rank\n",
    "    \n",
    "    def mock_is_available():\n",
    "        return True\n",
    "    \n",
    "    return patch.multiple(\n",
    "        dist,\n",
    "        get_world_size=mock_get_world_size,\n",
    "        get_rank=mock_get_rank,\n",
    "        is_available=mock_is_available\n",
    "    )\n",
    "\n",
    "# Test with 2 ranks (simulating 2 GPUs)\n",
    "world_size = 2\n",
    "batches_by_rank = {}\n",
    "\n",
    "for rank in range(world_size):\n",
    "    print(f\"\\n--- Testing Rank {rank}/{world_size-1} ---\")\n",
    "    \n",
    "    with mock_distributed_setup(world_size=world_size, rank=rank):\n",
    "        # Create distributed dataloader\n",
    "        train_dl_dist, eval_dl_dist = data.get_dataloaders(\n",
    "            128,  # train_batch_size\n",
    "            128,  # valid_batch_size\n",
    "            world_size,  # ngpus\n",
    "            1,    # accum\n",
    "            '/home/kkj/axolotl/datasets/IPR036736_90_grouped/train',\n",
    "            '/home/kkj/axolotl/datasets/IPR036736_90_grouped/valid',\n",
    "            1024, # max_length\n",
    "            False, # drop_last\n",
    "            0,    # num_workers (set to 0 for testing to avoid multiprocessing issues)\n",
    "            distributed=True,\n",
    "        )\n",
    "        \n",
    "        eval_iter_dist = iter(eval_dl_dist)\n",
    "        \n",
    "        # Collect first few batches for this rank\n",
    "        rank_batches = []\n",
    "        rank_total_tokens = 0\n",
    "        \n",
    "        for i, batch in enumerate(eval_iter_dist):\n",
    "            if i >= 10:  # Only test first 10 batches\n",
    "                break\n",
    "                \n",
    "            seq_lens = batch['input_ids'].offsets().diff()\n",
    "            total_tokens = seq_lens.sum().item()\n",
    "            rank_total_tokens += total_tokens\n",
    "            \n",
    "            rank_batches.append({\n",
    "                'batch_idx': i,\n",
    "                'num_sequences': len(seq_lens),\n",
    "                'total_tokens': total_tokens,\n",
    "                'seq_lengths': seq_lens.tolist()\n",
    "            })\n",
    "            \n",
    "            print(f\"  Batch {i}: {len(seq_lens)} sequences, {total_tokens} tokens\")\n",
    "        \n",
    "        batches_by_rank[rank] = {\n",
    "            'batches': rank_batches,\n",
    "            'total_tokens': rank_total_tokens\n",
    "        }\n",
    "        \n",
    "        print(f\"  Rank {rank} processed {len(rank_batches)} batches, {rank_total_tokens} total tokens\")\n",
    "\n",
    "# Analyze distribution\n",
    "print(f\"\\n--- Distribution Analysis ---\")\n",
    "print(f\"World size: {world_size}\")\n",
    "\n",
    "for rank in range(world_size):\n",
    "    rank_data = batches_by_rank[rank]\n",
    "    print(f\"Rank {rank}: {len(rank_data['batches'])} batches, {rank_data['total_tokens']} tokens\")\n",
    "\n",
    "# Check if ranks are getting different batches (they should be!)\n",
    "if world_size == 2:\n",
    "    rank0_tokens = [b['total_tokens'] for b in batches_by_rank[0]['batches']]\n",
    "    rank1_tokens = [b['total_tokens'] for b in batches_by_rank[1]['batches']]\n",
    "    \n",
    "    print(f\"\\nRank 0 token counts: {rank0_tokens[:5]}...\")\n",
    "    print(f\"Rank 1 token counts: {rank1_tokens[:5]}...\")\n",
    "    \n",
    "    # Check if they're different (which indicates proper distribution)\n",
    "    if rank0_tokens != rank1_tokens:\n",
    "        print(\"✓ Ranks are getting different batches (good!)\")\n",
    "    else:\n",
    "        print(\"⚠ Ranks are getting identical batches (potential issue)\")\n",
    "\n",
    "print(\"\\n--- Testing Epoch Cycling ---\")\n",
    "# Test epoch cycling with distributed setup\n",
    "with mock_distributed_setup(world_size=2, rank=0):\n",
    "    train_dl_cycle, _ = data.get_dataloaders(\n",
    "        64, 64, 2, 1,\n",
    "        '/home/kkj/axolotl/datasets/IPR036736_90_grouped/train',\n",
    "        '/home/kkj/axolotl/datasets/IPR036736_90_grouped/valid',\n",
    "        1024, False, 0, distributed=True\n",
    "    )\n",
    "    \n",
    "    train_iter_cycle = iter(train_dl_cycle)\n",
    "    \n",
    "    # Get a few batches and check epoch advancement\n",
    "    for i in range(5):\n",
    "        batch = next(train_iter_cycle)\n",
    "        seq_lens = batch['input_ids'].offsets().diff()\n",
    "        print(f\"Step {i}: Epoch ?, {len(seq_lens)} sequences, {seq_lens.sum()} tokens\")\n",
    "\n",
    "print(\"\\nDistributed testing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
