{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence lengths: [789, 862, 83]\n",
      "actual total: 1792\n",
      "total_length: 1792\n",
      "query_values.shape: torch.Size([1792, 4, 64])\n",
      "key_values.shape: torch.Size([1792, 4, 64])\n",
      "value_values.shape: torch.Size([1792, 4, 64])\n",
      "query.shape: torch.Size([4, j16, 4, 64])\n",
      "key.shape: torch.Size([4, j17, 4, 64])\n",
      "value.shape: torch.Size([4, j18, 4, 64])\n",
      "tensor([[[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]]], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "(1, 1, 1792, 1792)\n",
      "Correctness check passed ✅\n",
      "BlockMask(shape=(1, 1, 1792, 1792), sparsity=70.92%, \n",
      "(0, 0)\n",
      "██                          \n",
      "████                        \n",
      "██████                      \n",
      "████████                    \n",
      "██████████                  \n",
      "████████████                \n",
      "██████████████              \n",
      "            ████            \n",
      "            ██████          \n",
      "            ████████        \n",
      "            ██████████      \n",
      "            ████████████    \n",
      "            ██████████████  \n",
      "                        ████\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.attention.flex_attention import (\n",
    "    _DEFAULT_SPARSE_BLOCK_SIZE,\n",
    "    create_block_mask,\n",
    "    create_mask,\n",
    "    flex_attention,\n",
    ")\n",
    "from functools import lru_cache, partial\n",
    "\n",
    "@lru_cache\n",
    "def create_block_mask_cached(score_mod, B, H, M, N, device=\"cuda\"):\n",
    "    block_mask = create_block_mask(score_mod, B, H, M, N, device=device)\n",
    "    print(block_mask.to_dense())\n",
    "    print(block_mask.shape)\n",
    "    return block_mask\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "batch_size = 4\n",
    "n_heads = 4\n",
    "D = 64\n",
    "\n",
    "\n",
    "def prepare_qkv_values(tensor):\n",
    "    return tensor._values.detach().requires_grad_()\n",
    "\n",
    "\n",
    "def build_seq_idx(tensor: torch.Tensor):\n",
    "    offsets = tensor.offsets()\n",
    "    total_length = tensor.offsets()[-1].item()\n",
    "    print(\"total_length:\", total_length)\n",
    "    # Create a range tensor from 0 to total_length\n",
    "    range_tensor = torch.arange(total_length, device=\"cuda\", dtype=torch.int32)\n",
    "\n",
    "    # Use searchsorted to find the index for each position\n",
    "    seq_idx = torch.searchsorted(offsets, range_tensor, right=True) - 1\n",
    "\n",
    "    return seq_idx\n",
    "\n",
    "\n",
    "def create_njt_wrapper(orig_mask_mod, offsets, seq_idx):\n",
    "    \"\"\"Generic Wrapper that converts Dense mask_mod functions to NJT mask_mod functions\"\"\"\n",
    "\n",
    "    def njt_score_mod(b, h, q_idx, kv_idx):\n",
    "        q_nested = q_idx - offsets[seq_idx[q_idx]]\n",
    "        kv_nested = kv_idx - offsets[seq_idx[kv_idx]]\n",
    "        is_same_sequence = seq_idx[q_idx] == seq_idx[kv_idx]\n",
    "        return orig_mask_mod(b, h, q_nested, kv_nested) & is_same_sequence\n",
    "\n",
    "    return njt_score_mod\n",
    "\n",
    "\n",
    "# Dense Score Mod\n",
    "def causal_mask(b, h, q_idx, kv_idx):\n",
    "    return q_idx >= kv_idx\n",
    "    # return torch.where(q_idx >= kv_idx, score, -float(\"inf\"))\n",
    "\n",
    "\n",
    "# Current limitation that the total sequnce length must be divisible by 128\n",
    "sentence_lengths = [random.randint(1, 1024) for _ in range(batch_size - 1)]\n",
    "print(\"sentence lengths:\", sentence_lengths)\n",
    "total = sum(sentence_lengths)\n",
    "sentence_lengths.append(128 - total % 128)\n",
    "total = sum(sentence_lengths)\n",
    "print(\"actual total:\", total)\n",
    "\n",
    "ragged_tensors = [torch.randn(l, n_heads, D, device=\"cuda\") for l in sentence_lengths]\n",
    "query = torch.nested.nested_tensor(\n",
    "    ragged_tensors, layout=torch.jagged, requires_grad=True\n",
    ")\n",
    "key = torch.nested.nested_tensor(\n",
    "    ragged_tensors, layout=torch.jagged, requires_grad=True\n",
    ")\n",
    "value = torch.nested.nested_tensor(\n",
    "    ragged_tensors, layout=torch.jagged, requires_grad=True\n",
    ")\n",
    "\n",
    "# Build the seq_idx lookup table for\n",
    "offsets = query.offsets()\n",
    "seq_idx = build_seq_idx(query)\n",
    "\n",
    "causal_score_mod_njt = create_njt_wrapper(causal_mask, offsets, seq_idx)\n",
    "\n",
    "query_values = prepare_qkv_values(query)\n",
    "key_values = prepare_qkv_values(key)\n",
    "value_values = prepare_qkv_values(value)\n",
    "\n",
    "# print(\"query_values:\", query_values)\n",
    "# print(\"key_values:\", key_values)\n",
    "# print(\"value_values:\", value_values)\n",
    "\n",
    "print(\"query_values.shape:\", query_values.shape)\n",
    "print(\"key_values.shape:\", key_values.shape)\n",
    "print(\"value_values.shape:\", value_values.shape)\n",
    "\n",
    "# print(\"query:\", query)\n",
    "# print(\"key:\", key)\n",
    "# print(\"value:\", value)\n",
    "\n",
    "print(\"query.shape:\", query.shape)\n",
    "print(\"key.shape:\", key.shape)\n",
    "print(\"value.shape:\", value.shape)\n",
    "\n",
    "block_mask = create_block_mask_cached(\n",
    "    causal_score_mod_njt, 1, 1, total, total, device=query_values.device\n",
    ")\n",
    "out_flex = flex_attention(\n",
    "    query_values.view(1, -1, n_heads, D).transpose(1, 2),\n",
    "    key_values.view(1, -1, n_heads, D).transpose(1, 2),\n",
    "    value_values.view(1, -1, n_heads, D).transpose(1, 2),\n",
    "    block_mask=block_mask,\n",
    ")\n",
    "out_sdpa = F.scaled_dot_product_attention(\n",
    "    query.transpose(1, 2),\n",
    "    key.transpose(1, 2),\n",
    "    value.transpose(1, 2),\n",
    "    is_causal=True,\n",
    ")\n",
    "\n",
    "sdpa_outs = []\n",
    "flex_outs = []\n",
    "\n",
    "gradOut = torch.randn_like(out_sdpa)\n",
    "\n",
    "sdpa_outs.append(out_sdpa)\n",
    "out_sdpa.backward(gradOut)\n",
    "sdpa_outs += [query.grad, key.grad, value.grad]\n",
    "\n",
    "flex_outs.append(out_flex)\n",
    "out_flex.backward(gradOut._values.unsqueeze(0))\n",
    "flex_outs += [query_values.grad, key_values.grad, value_values.grad]\n",
    "\n",
    "for flex, sdpa in zip(flex_outs, sdpa_outs):\n",
    "    flex = flex.squeeze(0)\n",
    "    torch.testing.assert_close(flex, sdpa._values, atol=1e-2, rtol=1e-2)\n",
    "\n",
    "\n",
    "print(\"Correctness check passed ✅\")\n",
    "\n",
    "print(block_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence lengths: [789, 862, 83]\n",
      "query_values.shape: torch.Size([1792, 12, 64])\n",
      "key_values.shape: torch.Size([1792, 12, 64])\n",
      "value_values.shape: torch.Size([1792, 12, 64])\n",
      "query.shape: torch.Size([4, j13, 12, 64])\n",
      "key.shape: torch.Size([4, j14, 12, 64])\n",
      "value.shape: torch.Size([4, j15, 12, 64])\n",
      "Flex attention can run for a batch size of 4 with 12 heads, a dim of 64 and a sequence length sum of 1792\n",
      "BlockMask(shape=(1, 1, 1792, 1792), sparsity=48.98%, \n",
      "(0, 0)\n",
      "██████████████              \n",
      "██████████████              \n",
      "██████████████              \n",
      "██████████████              \n",
      "██████████████              \n",
      "██████████████              \n",
      "██████████████████████████  \n",
      "            ██████████████  \n",
      "            ██████████████  \n",
      "            ██████████████  \n",
      "            ██████████████  \n",
      "            ██████████████  \n",
      "            ████████████████\n",
      "                        ████\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "batch_size = 4\n",
    "n_heads = 12\n",
    "D = 64\n",
    "\n",
    "@lru_cache\n",
    "def create_block_mask_cached(mask_mod, B, H, M, N, device=\"cuda\"):\n",
    "    block_mask = create_block_mask(mask_mod, B, H, M, N, device=device)\n",
    "    return block_mask\n",
    "\n",
    "def prepare_qkv_values(tensor):\n",
    "    return tensor._values.detach().requires_grad_()\n",
    "\n",
    "def build_seq_idx(tensor: torch.Tensor):\n",
    "    offsets = tensor.offsets()\n",
    "    total_length = tensor.offsets()[-1].item()\n",
    "    # Create a range tensor from 0 to total_length\n",
    "    range_tensor = torch.arange(total_length, device=\"cuda\", dtype=torch.int32)\n",
    "\n",
    "    # Use searchsorted to find the index for each position\n",
    "    seq_idx = torch.searchsorted(offsets, range_tensor, right=True) - 1\n",
    "\n",
    "    return seq_idx\n",
    "\n",
    "def create_njt_wrapper(seq_idx):\n",
    "    \"\"\"Generic Wrapper that makes a NJT mask_mod\"\"\"\n",
    "\n",
    "    def njt_mask_mod(b, h, q_idx, kv_idx):\n",
    "        is_same_sequence = seq_idx[q_idx] == seq_idx[kv_idx]\n",
    "        return is_same_sequence\n",
    "\n",
    "    return njt_mask_mod\n",
    "\n",
    "# Current limitation that the total sequnce length must be divisible by 128\n",
    "sentence_lengths = [random.randint(1, 1024) for _ in range(batch_size - 1)]\n",
    "print(\"sentence lengths:\", sentence_lengths)\n",
    "total = sum(sentence_lengths)\n",
    "sentence_lengths.append(128 - total % 128)\n",
    "total = sum(sentence_lengths)\n",
    "\n",
    "ragged_tensors = [torch.randn(l, n_heads, D, device=\"cuda\") for l in sentence_lengths]\n",
    "query = torch.nested.nested_tensor(\n",
    "    ragged_tensors, layout=torch.jagged, requires_grad=True\n",
    ")\n",
    "key = torch.nested.nested_tensor(\n",
    "    ragged_tensors, layout=torch.jagged, requires_grad=True\n",
    ")\n",
    "value = torch.nested.nested_tensor(\n",
    "    ragged_tensors, layout=torch.jagged, requires_grad=True\n",
    ")\n",
    "\n",
    "# Build the seq_idx lookup table for\n",
    "offsets = query.offsets()\n",
    "seq_idx = build_seq_idx(query)\n",
    "\n",
    "mask_mod_njt = create_njt_wrapper(seq_idx)\n",
    "\n",
    "query_values = prepare_qkv_values(query)\n",
    "key_values = prepare_qkv_values(key)\n",
    "value_values = prepare_qkv_values(value)\n",
    "\n",
    "print(\"query_values.shape:\", query_values.shape)\n",
    "print(\"key_values.shape:\", key_values.shape)\n",
    "print(\"value_values.shape:\", value_values.shape)\n",
    "\n",
    "print(\"query.shape:\", query.shape)\n",
    "print(\"key.shape:\", key.shape)\n",
    "print(\"value.shape:\", value.shape)\n",
    "\n",
    "block_mask = create_block_mask_cached(\n",
    "    mask_mod_njt, 1, 1, total, total, device=query_values.device\n",
    ")\n",
    "out_flex = flex_attention(\n",
    "    query_values.view(1, -1, n_heads, D).transpose(1, 2),\n",
    "    key_values.view(1, -1, n_heads, D).transpose(1, 2),\n",
    "    value_values.view(1, -1, n_heads, D).transpose(1, 2),\n",
    "    block_mask=block_mask,\n",
    ")\n",
    "\n",
    "print(\"Flex attention can run for a batch size of\", batch_size, \"with\", n_heads, \"heads, a dim of\", D, \"and a sequence length sum of\", total)\n",
    "\n",
    "print(block_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
